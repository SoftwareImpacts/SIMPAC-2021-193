{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from augmentdata import data_augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install augmentdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cm_others(y_actual,y_predict):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm1 = confusion_matrix(y_actual,y_predict)\n",
    "    print('Confusion Matrix : \\n', cm1)\n",
    "\n",
    "    total1=sum(sum(cm1))\n",
    "    #####from confusion matrix calculate accuracy\n",
    "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "    print ('Accuracy : ', accuracy1)\n",
    "\n",
    "    tn=cm1[0,0]\n",
    "    fp=cm1[0,1]\n",
    "    fn=cm1[1,0]\n",
    "    tp=cm1[1,1]\n",
    "\n",
    "    sensitivity1 = tp/(tp+fn)\n",
    "    print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "    specificity1 = tn/(tn+fp)\n",
    "    print('Specificity : ', specificity1)\n",
    "    recall=sensitivity1\n",
    "    precision=tp/(tp+fp)\n",
    "\n",
    "    print(\"Precision = \",precision)\n",
    "    print(\"Recall = \",recall)\n",
    "\n",
    "    f1_score=2*(precision*recall)/(precision+recall)\n",
    "\n",
    "    print(\"F1 score = \",f1_score)\n",
    "\n",
    "    return sensitivity1,specificity1,f1_score\n",
    "\n",
    "def create_model(weight_path,input_dim):\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    model=None\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=input_dim, \n",
    "                activation='relu')) \n",
    "\n",
    "    model.add(Dense(20, \n",
    "                activation='relu'))\n",
    "    model.add(Dense(20, \n",
    "                activation='relu'))\n",
    "\n",
    "    model.add(Dense(20, \n",
    "                activation='relu'))\n",
    "\n",
    "    model.add(Dense(20, \n",
    "                activation='relu'))\n",
    "    model.add(Dense(20, \n",
    "                activation='relu'))\n",
    "\n",
    "    model.add(Dense(2, \n",
    "                activation='softmax'))\n",
    "    opt=keras.optimizers.Adam(lr=.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model, callbacks_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"data/Data1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 45)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Control    4000\n",
       "Case       2000\n",
       "Name: Case_Control, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Case_Control\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in df.iterrows():\n",
    "    if row[\"Case_Control\"] == \"Case\":\n",
    "        df.at[index,'Case_Control'] = 1\n",
    "    elif row[\"Case_Control\"] == \"Control\":\n",
    "        df.at[index,'Case_Control'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Case_Control'] = df['Case_Control'].astype(int)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4000\n",
       "1    2000\n",
       "Name: Case_Control, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Case_Control'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df['Nationality'] = df['Nationality'].astype(str)\n",
    "df.Nationality = label_encoder.fit_transform(df[\"Nationality\"])\n",
    "\n",
    "\n",
    "df['Gender'] = df['Gender'].astype(str)\n",
    "df.Gender = label_encoder.fit_transform(df[\"Gender\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['OccupationActivity'] = df['OccupationActivity'].astype(str)\n",
    "df.OccupationActivity = label_encoder.fit_transform(df[\"OccupationActivity\"])\n",
    "\n",
    "\n",
    "\n",
    "df['TimeSittingPerDay_Weekend_UsingComputer'] = df['TimeSittingPerDay_Weekend_UsingComputer'].astype(str)\n",
    "df.TimeSittingPerDay_Weekend_UsingComputer = label_encoder.fit_transform(df[\"TimeSittingPerDay_Weekend_UsingComputer\"])\n",
    "\n",
    "\n",
    "df['HeavySportActivities_Last7Days_AtLeast10Min'] = df['HeavySportActivities_Last7Days_AtLeast10Min'].astype(str)\n",
    "df.HeavySportActivities_Last7Days_AtLeast10Min = label_encoder.fit_transform(df[\"HeavySportActivities_Last7Days_AtLeast10Min\"])\n",
    "\n",
    "df['TimeWalked'] = df['TimeWalked'].astype(str)\n",
    "\n",
    "df.TimeWalked = label_encoder.fit_transform(df[\"TimeWalked\"])\n",
    "\n",
    "df['TypicalSleepHours_LastYear'] = df['TypicalSleepHours_LastYear'].astype(str)\n",
    "\n",
    "df.TypicalSleepHours_LastYear = label_encoder.fit_transform(df[\"TypicalSleepHours_LastYear\"])\n",
    "\n",
    "\n",
    "\n",
    "df['NapDuringDay_LastYear'] = df['NapDuringDay_LastYear'].astype(str)\n",
    "df.NapDuringDay_LastYear = label_encoder.fit_transform(df[\"NapDuringDay_LastYear\"])\n",
    "\n",
    "df['Snoar'] = df['Snoar'].astype(str)\n",
    "df.Snoar = label_encoder.fit_transform(df[\"Snoar\"])\n",
    "\n",
    "df['Smoke'] = df['Smoke'].astype(str)\n",
    "df.Smoke = label_encoder.fit_transform(df[\"Smoke\"])\n",
    "\n",
    "df['Age40_SmokingRate'] = df['Age40_SmokingRate'].astype(str)\n",
    "df.Age40_SmokingRate = label_encoder.fit_transform(df[\"Age40_SmokingRate\"])\n",
    "\n",
    "df['OtherHouseholdSmokers'] = df['OtherHouseholdSmokers'].astype(str)\n",
    "df.OtherHouseholdSmokers = label_encoder.fit_transform(df[\"OtherHouseholdSmokers\"])\n",
    "\n",
    "df['Shisha_Smoked'] = df['Shisha_Smoked'].astype(str)\n",
    "df.Shisha_Smoked = label_encoder.fit_transform(df[\"Shisha_Smoked\"])\n",
    "\n",
    "\n",
    "\n",
    "df['ParentsSmokedAtYourBirth'] = df['ParentsSmokedAtYourBirth'].astype(str)\n",
    "df.ParentsSmokedAtYourBirth = label_encoder.fit_transform(df[\"ParentsSmokedAtYourBirth\"])\n",
    "\n",
    "\n",
    "df['FatherAlive'] = df['FatherAlive'].astype(str)\n",
    "df.FatherAlive = label_encoder.fit_transform(df[\"FatherAlive\"])\n",
    "\n",
    "\n",
    "df['HIPWAIST_OUT_MEASURE_TAKEN'] = df['HIPWAIST_OUT_MEASURE_TAKEN'].astype(str)\n",
    "df.HIPWAIST_OUT_MEASURE_TAKEN = label_encoder.fit_transform(df[\"HIPWAIST_OUT_MEASURE_TAKEN\"])\n",
    "\n",
    "\n",
    "df['BP_OUT_ARM'] = df['BP_OUT_ARM'].astype(str)\n",
    "df.BP_OUT_ARM = label_encoder.fit_transform(df[\"BP_OUT_ARM\"])\n",
    "\n",
    "\n",
    "df['BP_OUT_MANUAL_ENRTY_YN_3'] = df['BP_OUT_MANUAL_ENRTY_YN_3'].astype(str)\n",
    "df.BP_OUT_MANUAL_ENRTY_YN_3 = label_encoder.fit_transform(df[\"BP_OUT_MANUAL_ENRTY_YN_3\"])\n",
    "\n",
    "\n",
    "df['BP_OUT_MANUAL_ENRTY_YN_2'] = df['BP_OUT_MANUAL_ENRTY_YN_2'].astype(str)\n",
    "df.BP_OUT_MANUAL_ENRTY_YN_2 = label_encoder.fit_transform(df[\"BP_OUT_MANUAL_ENRTY_YN_2\"])\n",
    "\n",
    "\n",
    "df['BP_OUT_ARMBAND_SIZE_USED'] = df['BP_OUT_ARMBAND_SIZE_USED'].astype(str)\n",
    "df.BP_OUT_ARMBAND_SIZE_USED = label_encoder.fit_transform(df[\"BP_OUT_ARMBAND_SIZE_USED\"])\n",
    "\n",
    "df['BP_OUT_MANUAL_ENRTY_YN_1'] = df['BP_OUT_MANUAL_ENRTY_YN_1'].astype(str)\n",
    "df.BP_OUT_MANUAL_ENRTY_YN_1 = label_encoder.fit_transform(df[\"BP_OUT_MANUAL_ENRTY_YN_1\"])\n",
    "\n",
    "\n",
    "df['BP_OUT_SLEEVE_TYPE'] = df['BP_OUT_SLEEVE_TYPE'].astype(str)\n",
    "df.BP_OUT_SLEEVE_TYPE = label_encoder.fit_transform(df[\"BP_OUT_SLEEVE_TYPE\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OccupationActivity</th>\n",
       "      <th>TimeSittingPerDay_Weekend_UsingComputer</th>\n",
       "      <th>TypicalSleepHours_LastYear</th>\n",
       "      <th>NapDuringDay_LastYear</th>\n",
       "      <th>Snoar</th>\n",
       "      <th>OtherHouseholdSmokers</th>\n",
       "      <th>FatherAlive</th>\n",
       "      <th>Age</th>\n",
       "      <th>HIPWAIST_OUT_WAIST_SIZE</th>\n",
       "      <th>HIPWAIST_OUT_HIPS_SIZE</th>\n",
       "      <th>BP_OUT_DIASTOLIC_BP_2</th>\n",
       "      <th>BP_OUT_DIASTOLIC_BP_1</th>\n",
       "      <th>BP_OUT_SYSTOLIC_BP_1</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Case_Control</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>115.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>87</td>\n",
       "      <td>139</td>\n",
       "      <td>37.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>118.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>81</td>\n",
       "      <td>136</td>\n",
       "      <td>37.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>82.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>58</td>\n",
       "      <td>187</td>\n",
       "      <td>27.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>91.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>76</td>\n",
       "      <td>130</td>\n",
       "      <td>33.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>90.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>73</td>\n",
       "      <td>119</td>\n",
       "      <td>35.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>91.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>65</td>\n",
       "      <td>102</td>\n",
       "      <td>26.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>98.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>64</td>\n",
       "      <td>112</td>\n",
       "      <td>32.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>99.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>73</td>\n",
       "      <td>115</td>\n",
       "      <td>26.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>98.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>82</td>\n",
       "      <td>113</td>\n",
       "      <td>35.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>110.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>79</td>\n",
       "      <td>138</td>\n",
       "      <td>33.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>85.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>69</td>\n",
       "      <td>111</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "      <td>96.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>61</td>\n",
       "      <td>107</td>\n",
       "      <td>27.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    OccupationActivity  TimeSittingPerDay_Weekend_UsingComputer  \\\n",
       "0                    0                                        5   \n",
       "1                    1                                        1   \n",
       "2                    2                                        5   \n",
       "3                    3                                        4   \n",
       "4                    0                                        1   \n",
       "5                    3                                        5   \n",
       "6                    0                                        5   \n",
       "7                    0                                        5   \n",
       "8                    0                                        5   \n",
       "9                    0                                        5   \n",
       "10                   0                                        2   \n",
       "11                   0                                        5   \n",
       "\n",
       "    TypicalSleepHours_LastYear  NapDuringDay_LastYear  Snoar  \\\n",
       "0                            3                      1      3   \n",
       "1                            4                      5      1   \n",
       "2                            3                      3      1   \n",
       "3                            2                      3      1   \n",
       "4                            3                      3      3   \n",
       "5                            2                      5      3   \n",
       "6                            3                      2      3   \n",
       "7                            2                      2      3   \n",
       "8                            3                      5      1   \n",
       "9                            4                      1      3   \n",
       "10                           2                      5      1   \n",
       "11                           2                      5      3   \n",
       "\n",
       "    OtherHouseholdSmokers  FatherAlive  Age  HIPWAIST_OUT_WAIST_SIZE  \\\n",
       "0                       3            2   52                    115.0   \n",
       "1                       1            2   55                    118.0   \n",
       "2                       3            2   80                     82.0   \n",
       "3                       4            3   28                     91.0   \n",
       "4                       3            2   51                     90.0   \n",
       "5                       3            3   44                     91.0   \n",
       "6                       1            2   56                     98.0   \n",
       "7                       4            2   55                     99.0   \n",
       "8                       1            3   40                     98.0   \n",
       "9                       3            2   56                    110.0   \n",
       "10                      1            3   42                     85.0   \n",
       "11                      1            2   63                     96.0   \n",
       "\n",
       "    HIPWAIST_OUT_HIPS_SIZE  BP_OUT_DIASTOLIC_BP_2  BP_OUT_DIASTOLIC_BP_1  \\\n",
       "0                    118.0                   84.0                     87   \n",
       "1                    117.0                   76.0                     81   \n",
       "2                     99.0                   69.0                     58   \n",
       "3                    114.0                   88.0                     76   \n",
       "4                    124.0                   78.0                     73   \n",
       "5                    101.0                   68.0                     65   \n",
       "6                    108.0                   59.0                     64   \n",
       "7                    104.0                   70.0                     73   \n",
       "8                    112.0                   83.0                     82   \n",
       "9                    105.0                   77.0                     79   \n",
       "10                    99.0                   76.0                     69   \n",
       "11                   102.0                   61.0                     61   \n",
       "\n",
       "    BP_OUT_SYSTOLIC_BP_1   BMI  Case_Control  \n",
       "0                    139  37.3             1  \n",
       "1                    136  37.1             1  \n",
       "2                    187  27.1             1  \n",
       "3                    130  33.8             1  \n",
       "4                    119  35.9             1  \n",
       "5                    102  26.7             1  \n",
       "6                    112  32.9             1  \n",
       "7                    115  26.6             1  \n",
       "8                    113  35.1             1  \n",
       "9                    138  33.3             1  \n",
       "10                   111  26.2             0  \n",
       "11                   107  27.5             1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[[\"Dummy ID\",\"OccupationActivity\"]].head(50)\n",
    "categorical_columns=[\"OccupationActivity\",\"TimeSittingPerDay_Weekend_UsingComputer\",\n",
    "                         \"TypicalSleepHours_LastYear\",\"NapDuringDay_LastYear\",\"Snoar\",\"OtherHouseholdSmokers\",\n",
    "                         \"FatherAlive\"\n",
    "                         ]\n",
    "\n",
    "# following columns after feature importance\n",
    "continuos_columns=[\"Age\",\"HIPWAIST_OUT_WAIST_SIZE\",\"HIPWAIST_OUT_HIPS_SIZE\",\n",
    "                   \"BP_OUT_DIASTOLIC_BP_2\",\"BP_OUT_DIASTOLIC_BP_1\",\"BP_OUT_SYSTOLIC_BP_1\",\n",
    "                   \"BMI\"]\n",
    "label_column=[\"Case_Control\"]                   \n",
    "all_useful_columns=categorical_columns+continuos_columns+label_column\n",
    "\n",
    "df[all_useful_columns].head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_na_500=[\"HeavySportActivities_Last7Days_AtLeast10Min\",\"TimeWalked\",\n",
    "                  \"DaysPerWeek_Walk\",\"TimePerDay_Walk\",\n",
    "                  \"Minutes_Walk\",\"Age40_SmokingRate\",\n",
    "                  \"HIPWAIST_OUT_MEASURE_TAKEN\",\"BP_OUT_SYSTOLIC_BP_3\",\n",
    "                  \"BP_OUT_PULSE_RATE_3\",\"BP_OUT_MANUAL_ENRTY_YN_3\",\n",
    "                  \"BP_OUT_DIASTOLIC_BP_3\",\"Smoke\",\"Shisha_Smoked\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=cols_with_na_500,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep interesting columns only\n",
    "\n",
    "df=df.drop(['Dummy ID'], axis=1)\n",
    "df=df.drop(columns=[\"HBA 1C %\"],axis=1)\n",
    "df=df.drop(columns=[\"Nationality\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    df[col]= df[col].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here take 20% out for testing\n",
    "np.random.seed(42)\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "train = df[msk]\n",
    "test = df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1199\n",
      "4801\n",
      "(1199, 29)\n",
      "(4801, 29)\n"
     ]
    }
   ],
   "source": [
    "print(len(test))\n",
    "print(len(train))\n",
    "print(test.shape)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    792\n",
      "1.0    407\n",
      "Name: Case_Control, dtype: int64\n",
      "0.0    3208\n",
      "1.0    1593\n",
      "Name: Case_Control, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test[\"Case_Control\"].value_counts())\n",
    "print(train[\"Case_Control\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Age', 'Gender', 'OccupationActivity',\n",
      "       'TimeSittingPerDay_Weekend_UsingComputer', 'TypicalSleepHours_LastYear',\n",
      "       'NapDuringDay_LastYear', 'Snoar', 'OtherHouseholdSmokers',\n",
      "       'ParentsSmokedAtYourBirth', 'FatherAlive', 'HIPWAIST_OUT_WAIST_SIZE',\n",
      "       'HIPWAIST_OUT_HIPS_SIZE', 'HIPWAIST_OUT_CALC_WAIST_TO_HIP_RATIO',\n",
      "       'BP_OUT_ARM', 'BP_OUT_SYSTOLIC_BP_2', 'BP_OUT_PULSE_RATE_2',\n",
      "       'BP_OUT_MANUAL_ENRTY_YN_2', 'BP_OUT_PULSE_RATE_1',\n",
      "       'BP_OUT_ARMBAND_SIZE_USED', 'BP_OUT_MANUAL_ENRTY_YN_1',\n",
      "       'BP_OUT_SLEEVE_TYPE', 'BP_OUT_DIASTOLIC_BP_2', 'BP_OUT_DIASTOLIC_BP_1',\n",
      "       'BP_OUT_SYSTOLIC_BP_1', 'BP_OUT_CALC_AVG_SYSTOLIC_BP',\n",
      "       'BP_OUT_CALC_AVG_DIASTOLIC_BP', 'BP_OUT_CALC_AVG_PULSE_RATE', 'BMI',\n",
      "       'Case_Control'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "columns=df.columns\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with no augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_results={}\n",
    "adasyn_results={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"0\"]={}\n",
    "smote_results[\"0\"]={}\n",
    "adasyn_results[\"0\"]={}\n",
    "source=train[train.columns[:-1]]\n",
    "print(source.shape)\n",
    "\n",
    "\n",
    "weight_path=\"weights/wt1.hdf5\"\n",
    "model,callbacks_list=create_model(weight_path,source.shape[1])\n",
    "target = list(train[\"Case_Control\"])\n",
    "target=pd.get_dummies(target)\n",
    "start=time.time()\n",
    "print(\"Training MLP\")\n",
    "history = model.fit(source.values, target,epochs=600,validation_split=0.2,callbacks=callbacks_list,verbose=0)\n",
    "end=time.time()\n",
    "difference = int(end - start)\n",
    "print(\"Time taken to train = \",difference)\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# load weights\n",
    "model.load_weights(weight_path)\n",
    "# Compile model (required to make predictions)\n",
    "opt=keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "print(\"used model and loaded weights from file\")\n",
    "\n",
    "print(\"Trying MLP\")\n",
    "results[\"0\"][\"MLP\"]={}\n",
    "smote_results[\"0\"][\"MLP\"]={}\n",
    "adasyn_results[\"0\"][\"MLP\"]={}\n",
    "\n",
    "y_actual = list(test[\"Case_Control\"])\n",
    "y_actual=pd.get_dummies(y_actual)\n",
    "test_features_only=test[test.columns[:-1]]\n",
    "_, accuracy = model.evaluate(test_features_only.values, y_actual)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "y_actual = test[\"Case_Control\"].astype(int)\n",
    "y_actual=np.asarray(y_actual)\n",
    "y_predict=model.predict_classes(test_features_only.values)\n",
    "sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "results[\"0\"][\"MLP\"][\"F1\"]=f1_score\n",
    "results[\"0\"][\"MLP\"][\"sensitivity\"]=sensitivity\n",
    "results[\"0\"][\"MLP\"][\"specificity\"]=specificity\n",
    "\n",
    "smote_results[\"0\"][\"MLP\"][\"F1\"]=f1_score  \n",
    "smote_results[\"0\"][\"MLP\"][\"sensitivity\"]=sensitivity\n",
    "smote_results[\"0\"][\"MLP\"][\"specificity\"]=specificity\n",
    "\n",
    "adasyn_results[\"0\"][\"MLP\"][\"F1\"]=f1_score  \n",
    "adasyn_results[\"0\"][\"MLP\"][\"sensitivity\"]=sensitivity\n",
    "adasyn_results[\"0\"][\"MLP\"][\"specificity\"]=specificity\n",
    "\n",
    "\n",
    "# for other classifiers\n",
    "target = list(train[\"Case_Control\"])\n",
    "\n",
    "# Random forest\n",
    "print(\"RF\")\n",
    "results[\"0\"][\"RF\"]={}\n",
    "smote_results[\"0\"][\"RF\"]={}\n",
    "adasyn_results[\"0\"][\"RF\"]={}\n",
    "\n",
    "clf=RandomForestClassifier()\n",
    "clf.fit(source.values,target)\n",
    "y_pred=None\n",
    "y_predict=clf.predict(test_features_only.values)\n",
    "sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "results[\"0\"][\"RF\"][\"F1\"]=f1_score\n",
    "results[\"0\"][\"RF\"][\"sensitivity\"]=sensitivity\n",
    "results[\"0\"][\"RF\"][\"specificity\"]=specificity\n",
    "\n",
    "smote_results[\"0\"][\"RF\"][\"F1\"]=f1_score  \n",
    "smote_results[\"0\"][\"RF\"][\"sensitivity\"]=sensitivity\n",
    "smote_results[\"0\"][\"RF\"][\"specificity\"]=specificity\n",
    "\n",
    "adasyn_results[\"0\"][\"RF\"][\"F1\"]=f1_score  \n",
    "adasyn_results[\"0\"][\"RF\"][\"sensitivity\"]=sensitivity\n",
    "adasyn_results[\"0\"][\"RF\"][\"specificity\"]=specificity\n",
    "\n",
    "#     Gaussian NB\n",
    "gnb = GaussianNB()\n",
    "results[\"0\"][\"GNB\"]={}\n",
    "smote_results[\"0\"][\"GNB\"]={}\n",
    "adasyn_results[\"0\"][\"GNB\"]={}\n",
    "\n",
    "print(\"GNB\")\n",
    "gnb.fit(source.values,target)\n",
    "y_predict = gnb.predict(test_features_only.values)\n",
    "sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "results[\"0\"][\"GNB\"][\"F1\"]=f1_score\n",
    "results[\"0\"][\"GNB\"][\"sensitivity\"]=sensitivity\n",
    "results[\"0\"][\"GNB\"][\"specificity\"]=specificity\n",
    "\n",
    "smote_results[\"0\"][\"GNB\"][\"F1\"]=f1_score  \n",
    "smote_results[\"0\"][\"GNB\"][\"sensitivity\"]=sensitivity\n",
    "smote_results[\"0\"][\"GNB\"][\"specificity\"]=specificity\n",
    "\n",
    "adasyn_results[\"0\"][\"GNB\"][\"F1\"]=f1_score  \n",
    "adasyn_results[\"0\"][\"GNB\"][\"sensitivity\"]=sensitivity\n",
    "adasyn_results[\"0\"][\"GNB\"][\"specificity\"]=specificity\n",
    "\n",
    "#     SVM\n",
    "clf = svm.SVC()\n",
    "results[\"0\"][\"SVM\"]={}\n",
    "smote_results[\"0\"][\"SVM\"]={}\n",
    "adasyn_results[\"0\"][\"SVM\"]={}\n",
    "\n",
    "print(\"SVM\")\n",
    "clf.fit(source.values,target)\n",
    "y_predict=clf.predict(test_features_only.values)\n",
    "sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "results[\"0\"][\"SVM\"][\"F1\"]=f1_score\n",
    "results[\"0\"][\"SVM\"][\"sensitivity\"]=sensitivity\n",
    "results[\"0\"][\"SVM\"][\"specificity\"]=specificity\n",
    "\n",
    "smote_results[\"0\"][\"SVM\"][\"F1\"]=f1_score  \n",
    "smote_results[\"0\"][\"SVM\"][\"sensitivity\"]=sensitivity\n",
    "smote_results[\"0\"][\"SVM\"][\"specificity\"]=specificity\n",
    "\n",
    "adasyn_results[\"0\"][\"SVM\"][\"F1\"]=f1_score  \n",
    "adasyn_results[\"0\"][\"SVM\"][\"sensitivity\"]=sensitivity\n",
    "adasyn_results[\"0\"][\"SVM\"][\"specificity\"]=specificity\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adasyn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Case_Control\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train  with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randmx =  1\n",
      "Time taken to augment =  111\n",
      "5301\n",
      "After augmentation of  500  items with  1  neighbors\n",
      "SVM\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_actual' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-847d4fa20b46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0my_predict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0msensitivity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspecificity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_cm_others\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_actual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SVM\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"F1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SVM\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sensitivity\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msensitivity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_actual' is not defined"
     ]
    }
   ],
   "source": [
    "N_range=[500,1000,1500]\n",
    "for N in N_range:\n",
    "    results[N]={}\n",
    "    smote_results[N]={}\n",
    "    adasyn_results[N]={}\n",
    "    \n",
    "    k_range=[1,2,5,10]\n",
    "    for k in k_range:\n",
    "        results[N][k]={}\n",
    "        smote_results[N][k]={}  \n",
    "        adasyn_results[N][k]={}\n",
    "        \n",
    "        results[N][k][\"MLP\"]={}\n",
    "        smote_results[N][k][\"MLP\"]={}  \n",
    "        adasyn_results[N][k][\"MLP\"]={}\n",
    "\n",
    "        results[N][k][\"RF\"]={}\n",
    "        smote_results[N][k][\"RF\"]={}  \n",
    "        adasyn_results[N][k][\"RF\"]={}\n",
    "        \n",
    "        results[N][k][\"GNB\"]={}\n",
    "        smote_results[N][k][\"GNB\"]={}  \n",
    "        adasyn_results[N][k][\"GNB\"]={}\n",
    "\n",
    "        results[N][k][\"SVM\"]={}\n",
    "        smote_results[N][k][\"SVM\"]={}  \n",
    "        adasyn_results[N][k][\"SVM\"]={}\n",
    "\n",
    "\n",
    "        class_index=1\n",
    "        randmx=1\n",
    "        dist_percent=0.2\n",
    "\n",
    "        \n",
    "        daug = data_augment.DataAugment()\n",
    "        print(\"randmx = \",randmx)\n",
    "        now = time.time()\n",
    "        [Data_a,Ext_d,Ext_not]=daug.augment(data=train.values,k=k,class_ind=class_index,N=N,\n",
    "                                            randmx=randmx,dist_percent=dist_percent)\n",
    "        later = time.time()\n",
    "        difference = int(later - now)\n",
    "        print(\"Time taken to augment = \",difference)\n",
    "        print(len(Data_a))\n",
    "\n",
    "        train_aug=pd.DataFrame(data=Data_a,index=None,    # values                \n",
    "                columns=columns)      \n",
    "\n",
    "        print(\"After augmentation of \",N,\" items with \",k,\" neighbors\")\n",
    "\n",
    "\n",
    "        \n",
    "        # this part for deep neural network\n",
    "        \n",
    "#         print(train_aug[\"Case_Control\"].value_counts())\n",
    "#         source=train_aug[train_aug.columns[:-1]]\n",
    "#         target = list(train_aug[\"Case_Control\"])\n",
    "#         target=pd.get_dummies(target)\n",
    "\n",
    "#         weight_path=\"weights/\"+str(N)+\"_\"+str(k)+\"_wt1.hdf5\"    \n",
    "#         model,callbacks_list=create_model(weight_path,source.shape[1])\n",
    "#         start=time.time()\n",
    "#         print(\"Training model for N = \",N,\" k = \",k)\n",
    "#         history = model.fit(source.values, target,epochs=600,validation_split=0.2,callbacks=callbacks_list,verbose=0)\n",
    "#         end=time.time()\n",
    "#         difference = int(end - start)\n",
    "#         print(\"Time taken to train = \",difference)\n",
    "\n",
    "\n",
    "\n",
    "#         plt.plot(history.history['loss'])\n",
    "#         plt.plot(history.history['val_loss'])\n",
    "#         plt.title('model loss')\n",
    "#         plt.ylabel('loss')\n",
    "#         plt.xlabel('epoch')\n",
    "#         plt.legend(['train','validation'], loc='upper left')\n",
    "#         plt.show()    \n",
    "\n",
    "#         # load weights\n",
    "#         model.load_weights(weight_path)\n",
    "#         # Compile model (required to make predictions)\n",
    "#         opt=keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "#         model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "#         print(\"used model and loaded weights from file\")    \n",
    "#         print(\"Test distribution\")\n",
    "#         print(test[\"Case_Control\"].value_counts())\n",
    "#         print(\"Trying MLP\")\n",
    "#         y_actual = list(test[\"Case_Control\"])\n",
    "#         y_actual=pd.get_dummies(y_actual)\n",
    "#         test_features_only=test[test.columns[:-1]]\n",
    "#         _, accuracy = model.evaluate(test_features_only.values, y_actual)\n",
    "#         print('Accuracy: %.2f' % (accuracy*100))            \n",
    "\n",
    "\n",
    "#         y_actual = test[\"Case_Control\"].astype(int)\n",
    "#         y_actual=np.asarray(y_actual)\n",
    "#         y_predict=model.predict_classes(test_features_only.values)\n",
    "#         sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "#         results[N][k][\"MLP\"][\"F1\"]=f1_score \n",
    "#         results[N][k][\"MLP\"][\"sensitivity\"]=sensitivity \n",
    "#         results[N][k][\"MLP\"][\"specificity\"]=specificity         \n",
    "        \n",
    "\n",
    "\n",
    "        # for other classifiers\n",
    "        target = list(train_aug[\"Case_Control\"])\n",
    "        source=train_aug[train_aug.columns[:-1]]\n",
    "        test_features_only=test[test.columns[:-1]]\n",
    "\n",
    "        # Random forest\n",
    "#         print(\"RF\")\n",
    "#         clf=RandomForestClassifier()\n",
    "#         clf.fit(source.values,target)\n",
    "#         y_predict=clf.predict(test_features_only.values)\n",
    "#         sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "#         results[N][k][\"RF\"][\"F1\"]=f1_score \n",
    "#         results[N][k][\"RF\"][\"sensitivity\"]=sensitivity \n",
    "#         results[N][k][\"RF\"][\"specificity\"]=specificity         \n",
    "                \n",
    "\n",
    "    #     Gaussian NB\n",
    "#         print(\"GNB\")\n",
    "#         gnb = GaussianNB()\n",
    "#         gnb.fit(source.values,target)\n",
    "#         y_predict = gnb.predict(test_features_only.values)\n",
    "#         sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "#         results[N][k][\"GNB\"][\"F1\"]=f1_score \n",
    "#         results[N][k][\"GNB\"][\"sensitivity\"]=sensitivity \n",
    "#         results[N][k][\"GNB\"][\"specificity\"]=specificity         \n",
    "                        \n",
    "        \n",
    "\n",
    "    #     SVM\n",
    "        print(\"SVM\")\n",
    "        clf = svm.SVC()\n",
    "        clf.fit(source.values,target)\n",
    "        y_predict=clf.predict(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        results[N][k][\"SVM\"][\"F1\"]=f1_score \n",
    "        results[N][k][\"SVM\"][\"sensitivity\"]=sensitivity \n",
    "        results[N][k][\"SVM\"][\"specificity\"]=specificity         \n",
    "                                \n",
    "        \n",
    "        \n",
    "        \n",
    "        # this part for SMOTE\n",
    "        print(\"******Starting SMOTE***********\")\n",
    "        source=train[train.columns[:-1]]\n",
    "        print(\"Shape of training \",source.shape)\n",
    "\n",
    "        y_train=train[\"Case_Control\"].values\n",
    "        num_minority=sum(y_train==class_index)\n",
    "        num_majority=len(y_train)-num_minority\n",
    "        print(num_majority,num_minority)\n",
    "        \n",
    "        samp_strategy=(num_minority+N)/num_majority\n",
    "        print(samp_strategy)\n",
    "        now = time.time()\n",
    "        sm = SMOTE(random_state=2,k_neighbors=k,sampling_strategy=samp_strategy)\n",
    "        X_train_res, y_train_res = sm.fit_sample(source, y_train.ravel())\n",
    "        source=X_train_res\n",
    "\n",
    "        later = time.time()\n",
    "        difference = int(later - now)\n",
    "        print(\"Time taken to augment by SMOTE = \",difference)\n",
    "\n",
    "        print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
    "        print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
    "\n",
    "        print(\"After OverSampling, counts of label {}: {}\".format(class_index,sum(y_train_res==1)))\n",
    "        print(\"After OverSampling, counts of other label: {}\".format(len(y_train_res)-sum(y_train_res==class_index)))  \n",
    "        \n",
    "\n",
    "        # this pert for dnn\n",
    "        \n",
    "#         weight_path=\"weights/smote\"+\"_\"+str(N)+\"_\"+str(k)+\"_wt1.hdf5\"    \n",
    "#         model,callbacks_list=create_model(weight_path,X_train_res.shape[1])\n",
    "#         start=time.time()\n",
    "#         print(\"Training model\")\n",
    "#         target=pd.get_dummies(y_train_res)\n",
    "#         history = model.fit(X_train_res.values, target,epochs=600,validation_split=0.2,callbacks=callbacks_list,verbose=0)\n",
    "#         end=time.time()\n",
    "#         difference = int(end - start)\n",
    "#         print(\"Time taken to train = \",difference) \n",
    "\n",
    "#         plt.plot(history.history['loss'])\n",
    "#         plt.plot(history.history['val_loss'])\n",
    "#         plt.title('model loss')\n",
    "#         plt.ylabel('loss')\n",
    "#         plt.xlabel('epoch')\n",
    "#         plt.legend(['train','validation'], loc='upper left')\n",
    "#         plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "#         # load weights\n",
    "#         model.load_weights(weight_path)\n",
    "#         # Compile model (required to make predictions)\n",
    "#         opt=keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "#         model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "#         print(\"used model and loaded weights from file\")    \n",
    "#         print(\"Trying MLP\")\n",
    "#         y_actual = list(test[\"Case_Control\"])\n",
    "#         y_actual=pd.get_dummies(y_actual)\n",
    "#         test_features_only=test[test.columns[:-1]]\n",
    "#         _, accuracy = model.evaluate(test_features_only.values, y_actual)\n",
    "#         print('Accuracy: %.2f' % (accuracy*100)) \n",
    "\n",
    "\n",
    "\n",
    "#         y_actual = test[\"Case_Control\"].astype(int)\n",
    "#         y_actual=np.asarray(y_actual)\n",
    "#         y_predict=model.predict_classes(test_features_only.values)\n",
    "#         sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "#         smote_results[N][k][\"MLP\"][\"F1\"]=f1_score         \n",
    "#         smote_results[N][k][\"MLP\"][\"sensitivity\"]=sensitivity                 \n",
    "#         smote_results[N][k][\"MLP\"][\"specificity\"]=specificity                 \n",
    "\n",
    "        # for other classifiers\n",
    "        target = y_train_res\n",
    "        source=X_train_res\n",
    "        test_features_only=test[test.columns[:-1]]\n",
    "\n",
    "        # Random forest\n",
    "#         print(\"RF\")\n",
    "#         clf=RandomForestClassifier()\n",
    "#         clf.fit(source.values,target)\n",
    "#         y_predict=clf.predict(test_features_only.values)\n",
    "#         sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "#         smote_results[N][k][\"RF\"][\"F1\"]=f1_score         \n",
    "#         smote_results[N][k][\"RF\"][\"sensitivity\"]=sensitivity                 \n",
    "#         smote_results[N][k][\"RF\"][\"specificity\"]=specificity\n",
    "\n",
    "#     #     Gaussian NB\n",
    "#         print(\"GNB\")\n",
    "#         gnb = GaussianNB()\n",
    "#         gnb.fit(source.values,target)\n",
    "#         y_predict = gnb.predict(test_features_only.values)\n",
    "#         sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "#         smote_results[N][k][\"GNB\"][\"F1\"]=f1_score         \n",
    "#         smote_results[N][k][\"GNB\"][\"sensitivity\"]=sensitivity                 \n",
    "#         smote_results[N][k][\"GNB\"][\"specificity\"]=specificity\n",
    "\n",
    "    #     SVM\n",
    "        print(\"SVM\")\n",
    "        clf = svm.SVC()\n",
    "        clf.fit(source.values,target)\n",
    "        y_predict=clf.predict(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        smote_results[N][k][\"SVM\"][\"F1\"]=f1_score         \n",
    "        smote_results[N][k][\"SVM\"][\"sensitivity\"]=sensitivity                 \n",
    "        smote_results[N][k][\"SVM\"][\"specificity\"]=specificity\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # this part for ADASYN\n",
    "        print(\"******Starting ADASYN***********\")\n",
    "        source=train[train.columns[:-1]]\n",
    "        print(\"Shape of training \",source.shape)\n",
    "\n",
    "        y_train=train[\"Case_Control\"].values\n",
    "        num_minority=sum(y_train==class_index)\n",
    "        num_majority=len(y_train)-num_minority\n",
    "        print(num_majority,num_minority)\n",
    "        \n",
    "        samp_strategy=(num_minority+N)/num_majority\n",
    "        print(samp_strategy)\n",
    "        now = time.time()\n",
    "        ad = ADASYN(random_state=2,n_neighbors=k,sampling_strategy=samp_strategy)\n",
    "        X_train_res, y_train_res = ad.fit_sample(source, y_train.ravel())\n",
    "        source=X_train_res\n",
    "\n",
    "        later = time.time()\n",
    "        difference = int(later - now)\n",
    "        print(\"Time taken to augment by ADASYN = \",difference)\n",
    "\n",
    "        print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
    "        print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
    "\n",
    "        print(\"After OverSampling, counts of label {}: {}\".format(class_index,sum(y_train_res==1)))\n",
    "        print(\"After OverSampling, counts of other label: {}\".format(len(y_train_res)-sum(y_train_res==class_index)))  \n",
    "        \n",
    "        \n",
    "        # this part for DNN\n",
    "        \n",
    "#         weight_path=\"weights/adasyn\"+\"_\"+str(N)+\"_\"+str(k)+\"_wt1.hdf5\"    \n",
    "#         model,callbacks_list=create_model(weight_path,X_train_res.shape[1])\n",
    "#         start=time.time()\n",
    "#         print(\"Training model\")\n",
    "#         target=pd.get_dummies(y_train_res)\n",
    "#         history = model.fit(X_train_res.values, target,epochs=600,validation_split=0.2,callbacks=callbacks_list,verbose=0)\n",
    "#         end=time.time()\n",
    "#         difference = int(end - start)\n",
    "#         print(\"Time taken to train = \",difference) \n",
    "\n",
    "#         plt.plot(history.history['loss'])\n",
    "#         plt.plot(history.history['val_loss'])\n",
    "#         plt.title('model loss')\n",
    "#         plt.ylabel('loss')\n",
    "#         plt.xlabel('epoch')\n",
    "#         plt.legend(['train','validation'], loc='upper left')\n",
    "#         plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "#         # load weights\n",
    "#         model.load_weights(weight_path)\n",
    "#         # Compile model (required to make predictions)\n",
    "#         opt=keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "#         model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "#         print(\"used model and loaded weights from file\")    \n",
    "#         print(\"Trying MLP\")\n",
    "#         y_actual = list(test[\"Case_Control\"])\n",
    "#         y_actual=pd.get_dummies(y_actual)\n",
    "#         test_features_only=test[test.columns[:-1]]\n",
    "#         _, accuracy = model.evaluate(test_features_only.values, y_actual)\n",
    "#         print('Accuracy: %.2f' % (accuracy*100)) \n",
    "\n",
    "\n",
    "\n",
    "#         y_actual = test[\"Case_Control\"].astype(int)\n",
    "#         y_actual=np.asarray(y_actual)\n",
    "#         y_predict=model.predict_classes(test_features_only.values)\n",
    "#         sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "#         adasyn_results[N][k][\"MLP\"][\"F1\"]=f1_score         \n",
    "#         adasyn_results[N][k][\"MLP\"][\"sensitivity\"]=sensitivity                 \n",
    "#         adasyn_results[N][k][\"MLP\"][\"specificity\"]=specificity\n",
    "\n",
    "        # for other classifiers\n",
    "        target = y_train_res\n",
    "        source=X_train_res\n",
    "        test_features_only=test[test.columns[:-1]]\n",
    "\n",
    "        # Random forest\n",
    "#         print(\"RF\")\n",
    "#         clf=RandomForestClassifier()\n",
    "#         clf.fit(source.values,target)\n",
    "#         y_predict=clf.predict(test_features_only.values)\n",
    "#         sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "#         adasyn_results[N][k][\"RF\"][\"F1\"]=f1_score         \n",
    "#         adasyn_results[N][k][\"RF\"][\"sensitivity\"]=sensitivity                 \n",
    "#         adasyn_results[N][k][\"RF\"][\"specificity\"]=specificity\n",
    "\n",
    "#     #     Gaussian NB\n",
    "#         print(\"GNB\")\n",
    "#         gnb = GaussianNB()\n",
    "#         gnb.fit(source.values,target)\n",
    "#         y_predict = gnb.predict(test_features_only.values)\n",
    "#         sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "#         adasyn_results[N][k][\"GNB\"][\"F1\"]=f1_score         \n",
    "#         adasyn_results[N][k][\"GNB\"][\"sensitivity\"]=sensitivity                 \n",
    "#         adasyn_results[N][k][\"GNB\"][\"specificity\"]=specificity\n",
    "\n",
    "    #     SVM\n",
    "        print(\"SVM\")\n",
    "        clf = svm.SVC()\n",
    "        clf.fit(source.values,target)\n",
    "        y_predict=clf.predict(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        adasyn_results[N][k][\"SVM\"][\"F1\"]=f1_score         \n",
    "        adasyn_results[N][k][\"SVM\"][\"sensitivity\"]=sensitivity                 \n",
    "        adasyn_results[N][k][\"SVM\"][\"specificity\"]=specificity        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best\n",
    "\n",
    "print(\"randmx = \",randmx)\n",
    "print(\"dist_percent = \",dist_percent)\n",
    "import math\n",
    "checks=[\"MLP\",\"RF\",\"GNB\",\"SVM\"]\n",
    "# checks=[\"GNB\"]\n",
    "\n",
    "best_n=None\n",
    "best_k=None\n",
    "best_sensitivity=0\n",
    "best_specificity=0\n",
    "best_f1=0\n",
    "\n",
    "print(\"KNNOR\")\n",
    "\n",
    "\n",
    "for check in checks:\n",
    "    for k,v in results.items():\n",
    "        if k==\"0\":\n",
    "            continue\n",
    "        for k1,v1 in v.items():\n",
    "#             print(k1)\n",
    "#             print(v1)\n",
    "            if math.isnan(v1[check][\"F1\"]):\n",
    "                continue\n",
    "#             print(v1[check][\"F1\"])\n",
    "            if best_f1<v1[check][\"F1\"]:\n",
    "                best_f1=v1[check][\"F1\"]\n",
    "                best_n=k\n",
    "                best_k=k1\n",
    "                best_sensitivity=v1[check][\"sensitivity\"]\n",
    "                best_specificity=v1[check][\"specificity\"]\n",
    "                \n",
    "\n",
    "    print(check,best_f1,\"[N=\",best_n,\",k=\",best_k,\"]\")\n",
    "    print(\"Sensitivity = \",best_sensitivity)\n",
    "    print(\"Specificity = \",best_specificity)    \n",
    "    \n",
    "    best_f1=0\n",
    "    best_k=None\n",
    "    best_n=None\n",
    "    best_sensitivity=0\n",
    "    best_specificity=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best\n",
    "\n",
    "print(\"randmx = \",randmx)\n",
    "print(\"dist_percent = \",dist_percent)\n",
    "import math\n",
    "checks=[\"MLP\",\"RF\",\"GNB\",\"SVM\"]\n",
    "# checks=[\"GNB\"]\n",
    "\n",
    "best_n=None\n",
    "best_k=None\n",
    "best_sensitivity=0\n",
    "best_specificity=0\n",
    "best_f1=0\n",
    "\n",
    "print(\"SMOTE\")\n",
    "\n",
    "\n",
    "for check in checks:\n",
    "    for k,v in smote_results.items():\n",
    "        if k==\"0\":\n",
    "            continue\n",
    "        for k1,v1 in v.items():\n",
    "#             print(k1)\n",
    "#             print(v1)\n",
    "            if math.isnan(v1[check][\"F1\"]):\n",
    "                continue\n",
    "#             print(v1[check][\"F1\"])\n",
    "            if best_f1<v1[check][\"F1\"]:\n",
    "                best_f1=v1[check][\"F1\"]\n",
    "                best_n=k\n",
    "                best_k=k1\n",
    "                best_sensitivity=v1[check][\"sensitivity\"]\n",
    "                best_specificity=v1[check][\"specificity\"]                \n",
    "\n",
    "    print(check,best_f1,\"[N=\",best_n,\",k=\",best_k,\"]\")\n",
    "    print(\"Sensitivity = \",best_sensitivity)\n",
    "    print(\"Specificity = \",best_specificity)\n",
    "    \n",
    "    best_f1=0\n",
    "    best_k=None\n",
    "    best_n=None\n",
    "    best_sensitivity=0\n",
    "    best_specificity=0\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best\n",
    "\n",
    "print(\"randmx = \",randmx)\n",
    "print(\"dist_percent = \",dist_percent)\n",
    "import math\n",
    "checks=[\"MLP\",\"RF\",\"GNB\",\"SVM\"]\n",
    "# checks=[\"GNB\"]\n",
    "\n",
    "best_n=None\n",
    "best_k=None\n",
    "best_sensitivity=0\n",
    "best_specificity=0\n",
    "best_f1=0\n",
    "\n",
    "print(\"ADASYN\")\n",
    "\n",
    "\n",
    "for check in checks:\n",
    "    for k,v in adasyn_results.items():\n",
    "        if k==\"0\":\n",
    "            continue\n",
    "        for k1,v1 in v.items():\n",
    "#             print(k1)\n",
    "#             print(v1)\n",
    "            if math.isnan(v1[check][\"F1\"]):\n",
    "                continue\n",
    "#             print(v1[check][\"F1\"])\n",
    "            if best_f1<v1[check][\"F1\"]:\n",
    "                best_f1=v1[check][\"F1\"]\n",
    "                best_n=k\n",
    "                best_k=k1\n",
    "                best_sensitivity=v1[check][\"sensitivity\"]\n",
    "                best_specificity=v1[check][\"specificity\"]                \n",
    "\n",
    "    print(check,best_f1,\"[N=\",best_n,\",k=\",best_k,\"]\")\n",
    "    print(\"Sensitivity = \",best_sensitivity)\n",
    "    print(\"Specificity = \",best_specificity)\n",
    "    \n",
    "    best_f1=0\n",
    "    best_k=None\n",
    "    best_n=None\n",
    "    best_sensitivity=0\n",
    "    best_specificity=0\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this part for trying with specific values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randmx =  0.12\n",
      "dist_percent =  0.5\n",
      "6301\n",
      "SVM\n",
      "Confusion Matrix : \n",
      " [[576 216]\n",
      " [ 86 321]]\n",
      "Accuracy :  0.7481234361968306\n",
      "Sensitivity :  0.7886977886977887\n",
      "Specificity :  0.7272727272727273\n",
      "Precision =  0.5977653631284916\n",
      "Recall =  0.7886977886977887\n",
      "F1 score =  0.6800847457627118\n",
      "*************************************\n",
      "N= 1500\n",
      "k= 2\n",
      "randmx 0.12\n",
      "dist_percent 0.5\n",
      "f1 0.6800847457627118\n",
      "sensitivity 0.7886977886977887\n",
      "specificity 0.7272727272727273\n",
      "randmx =  0.12\n",
      "dist_percent =  0.5\n",
      "6301\n",
      "SVM\n",
      "Confusion Matrix : \n",
      " [[577 215]\n",
      " [ 86 321]]\n",
      "Accuracy :  0.7489574645537949\n",
      "Sensitivity :  0.7886977886977887\n",
      "Specificity :  0.7285353535353535\n",
      "Precision =  0.5988805970149254\n",
      "Recall =  0.7886977886977887\n",
      "F1 score =  0.6808059384941675\n",
      "*************************************\n",
      "N= 1500\n",
      "k= 2\n",
      "randmx 0.12\n",
      "dist_percent 0.5\n",
      "f1 0.6808059384941675\n",
      "sensitivity 0.7886977886977887\n",
      "specificity 0.7285353535353535\n",
      "randmx =  0.12\n",
      "dist_percent =  0.5\n",
      "6301\n",
      "SVM\n",
      "Confusion Matrix : \n",
      " [[576 216]\n",
      " [ 86 321]]\n",
      "Accuracy :  0.7481234361968306\n",
      "Sensitivity :  0.7886977886977887\n",
      "Specificity :  0.7272727272727273\n",
      "Precision =  0.5977653631284916\n",
      "Recall =  0.7886977886977887\n",
      "F1 score =  0.6800847457627118\n",
      "*************************************\n",
      "N= 1500\n",
      "k= 2\n",
      "randmx 0.12\n",
      "dist_percent 0.5\n",
      "f1 0.6800847457627118\n",
      "sensitivity 0.7886977886977887\n",
      "specificity 0.7272727272727273\n",
      "randmx =  0.12\n",
      "dist_percent =  0.5\n",
      "6301\n",
      "SVM\n",
      "Confusion Matrix : \n",
      " [[577 215]\n",
      " [ 84 323]]\n",
      "Accuracy :  0.7506255212677231\n",
      "Sensitivity :  0.7936117936117936\n",
      "Specificity :  0.7285353535353535\n",
      "Precision =  0.6003717472118959\n",
      "Recall =  0.7936117936117936\n",
      "F1 score =  0.6835978835978836\n",
      "*************************************\n",
      "N= 1500\n",
      "k= 2\n",
      "randmx 0.12\n",
      "dist_percent 0.5\n",
      "f1 0.6835978835978836\n",
      "sensitivity 0.7936117936117936\n",
      "specificity 0.7285353535353535\n",
      "randmx =  0.12\n",
      "dist_percent =  0.5\n",
      "6301\n",
      "SVM\n",
      "Confusion Matrix : \n",
      " [[577 215]\n",
      " [ 84 323]]\n",
      "Accuracy :  0.7506255212677231\n",
      "Sensitivity :  0.7936117936117936\n",
      "Specificity :  0.7285353535353535\n",
      "Precision =  0.6003717472118959\n",
      "Recall =  0.7936117936117936\n",
      "F1 score =  0.6835978835978836\n",
      "*************************************\n",
      "N= 1500\n",
      "k= 2\n",
      "randmx 0.12\n",
      "dist_percent 0.5\n",
      "f1 0.6835978835978836\n",
      "sensitivity 0.7936117936117936\n",
      "specificity 0.7285353535353535\n",
      "randmx =  0.12\n",
      "dist_percent =  0.5\n",
      "6301\n",
      "SVM\n",
      "Confusion Matrix : \n",
      " [[577 215]\n",
      " [ 85 322]]\n",
      "Accuracy :  0.749791492910759\n",
      "Sensitivity :  0.7911547911547911\n",
      "Specificity :  0.7285353535353535\n",
      "Precision =  0.5996275605214153\n",
      "Recall =  0.7911547911547911\n",
      "F1 score =  0.6822033898305084\n",
      "*************************************\n",
      "N= 1500\n",
      "k= 2\n",
      "randmx 0.12\n",
      "dist_percent 0.5\n",
      "f1 0.6822033898305084\n",
      "sensitivity 0.7911547911547911\n",
      "specificity 0.7285353535353535\n",
      "randmx =  0.12\n",
      "dist_percent =  0.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-eb731a5f3187>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0mnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     [Data_a,Ext_d,Ext_not]=daug.augment(data=train.values,k=k,class_ind=class_index,N=N,\n\u001b[0;32m---> 33\u001b[0;31m                                                         randmx=randmx,dist_percent=dist_percent)\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0mlater\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mdifference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlater\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/hbku/spring_2020/independednt_Study_Prof_Samir/venv_indept_study/lib/python3.6/site-packages/augmentdata/data_augment.py\u001b[0m in \u001b[0;36maugment\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    249\u001b[0m                                                 \u001b[0;31m# for testing the artificial data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                                                 \u001b[0mcan_use\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neighbors_to_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m                                                 \u001b[0;31m# print(\"Usability \",can_use)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/hbku/spring_2020/independednt_Study_Prof_Samir/venv_indept_study/lib/python3.6/site-packages/augmentdata/test_nn.py\u001b[0m in \u001b[0;36mpredict_classification\u001b[0;34m(train, test_row, num_neighbors, test_val)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Make a classification prediction with neighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neighbors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mneighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moutput_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# print(output_values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/hbku/spring_2020/independednt_Study_Prof_Samir/venv_indept_study/lib/python3.6/site-packages/augmentdata/test_nn.py\u001b[0m in \u001b[0;36mget_neighbors\u001b[0;34m(train, test_row, num_neighbors)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_row\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuclidean_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/hbku/spring_2020/independednt_Study_Prof_Samir/venv_indept_study/lib/python3.6/site-packages/augmentdata/test_nn.py\u001b[0m in \u001b[0;36meuclidean_distance\u001b[0;34m(row1, row2)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0mdistance\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrow2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "#     N_range=[500,1000,1500]\n",
    "    N_range=[1500]\n",
    "    for N in N_range:\n",
    "        results[N]={}\n",
    "        smote_results[N]={}\n",
    "        adasyn_results[N]={}\n",
    "\n",
    "    #     k_range=[1,2,5,10]\n",
    "        k_range=[2]    \n",
    "        for k in k_range:\n",
    "\n",
    "#             randmx_range=[0.01,0.05,0.1,0.5,0.9]\n",
    "            randmx_range=[0.12]\n",
    "\n",
    "            for randmx in randmx_range:\n",
    "#                 dist_percent_range=[0.01,0.05,0.1,0.5,0.9]\n",
    "                dist_percent_range=[0.5]\n",
    "                for dist_percent in dist_percent_range:\n",
    "\n",
    "\n",
    "\n",
    "                    class_index=1\n",
    "        #             randmx=0.12\n",
    "                    dist_percent=0.5\n",
    "\n",
    "\n",
    "                    daug = data_augment.DataAugment()\n",
    "                    print(\"randmx = \",randmx)\n",
    "                    print(\"dist_percent = \",dist_percent)        \n",
    "                    now = time.time()\n",
    "                    [Data_a,Ext_d,Ext_not]=daug.augment(data=train.values,k=k,class_ind=class_index,N=N,\n",
    "                                                        randmx=randmx,dist_percent=dist_percent)\n",
    "                    later = time.time()\n",
    "                    difference = int(later - now)\n",
    "            #         print(\"Time taken to augment = \",difference)\n",
    "                    print(len(Data_a))\n",
    "\n",
    "                    train_aug=pd.DataFrame(data=Data_a,index=None,    # values                \n",
    "                            columns=columns)      \n",
    "\n",
    "            #         print(\"After augmentation of \",N,\" items with \",k,\" neighbors\")\n",
    "\n",
    "            #         print(train_aug[\"Case_Control\"].value_counts())\n",
    "            #         source=train_aug[train_aug.columns[:-1]]\n",
    "            #         target = list(train_aug[\"Case_Control\"])\n",
    "            #         target=pd.get_dummies(target)\n",
    "\n",
    "            #         weight_path=\"weights/\"+str(N)+\"_\"+str(k)+\"_wt1.hdf5\"    \n",
    "            #         model,callbacks_list=create_model(weight_path,source.shape[1])\n",
    "            #         start=time.time()\n",
    "            #         print(\"Training model for N = \",N,\" k = \",k)\n",
    "            #         history = model.fit(source.values, target,epochs=600,validation_split=0.2,callbacks=callbacks_list,verbose=0)\n",
    "            #         end=time.time()\n",
    "            #         difference = int(end - start)\n",
    "            #         print(\"Time taken to train = \",difference)\n",
    "\n",
    "\n",
    "\n",
    "            #         plt.plot(history.history['loss'])\n",
    "            #         plt.plot(history.history['val_loss'])\n",
    "            #         plt.title('model loss')\n",
    "            #         plt.ylabel('loss')\n",
    "            #         plt.xlabel('epoch')\n",
    "            #         plt.legend(['train','validation'], loc='upper left')\n",
    "            #         plt.show()    \n",
    "\n",
    "            #         # load weights\n",
    "            #         model.load_weights(weight_path)\n",
    "            #         # Compile model (required to make predictions)\n",
    "            #         opt=keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "            #         model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "            #         print(\"used model and loaded weights from file\")    \n",
    "            #         print(\"Test distribution\")\n",
    "            #         print(test[\"Case_Control\"].value_counts())\n",
    "            #         print(\"Trying MLP\")\n",
    "            #         y_actual = list(test[\"Case_Control\"])\n",
    "            #         y_actual=pd.get_dummies(y_actual)\n",
    "            #         test_features_only=test[test.columns[:-1]]\n",
    "            #         _, accuracy = model.evaluate(test_features_only.values, y_actual)\n",
    "            #         print('Accuracy: %.2f' % (accuracy*100))            \n",
    "\n",
    "\n",
    "                    y_actual = test[\"Case_Control\"].astype(int)\n",
    "                    y_actual=np.asarray(y_actual)\n",
    "            #         y_predict=model.predict_classes(test_features_only.values)\n",
    "            #         sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "\n",
    "\n",
    "                    # for other classifiers\n",
    "                    target = list(train_aug[\"Case_Control\"])\n",
    "                    source=train_aug[train_aug.columns[:-1]]\n",
    "                    test_features_only=test[test.columns[:-1]]\n",
    "\n",
    "                    # Random forest\n",
    "            #         print(\"RF\")\n",
    "            #         clf=RandomForestClassifier()\n",
    "            #         clf.fit(source.values,target)\n",
    "            #         y_predict=clf.predict(test_features_only.values)\n",
    "            #         sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "\n",
    "\n",
    "                #     Gaussian NB\n",
    "            #         print(\"GNB\")\n",
    "            #         gnb = GaussianNB()\n",
    "            #         gnb.fit(source.values,target)\n",
    "            #         y_predict = gnb.predict(test_features_only.values)\n",
    "            #         sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "\n",
    "\n",
    "\n",
    "            #     #     SVM\n",
    "                    print(\"SVM\")\n",
    "                    clf = svm.SVC()\n",
    "                    clf.fit(source.values,target)\n",
    "                    y_predict=clf.predict(test_features_only.values)\n",
    "                    sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "\n",
    "\n",
    "\n",
    "                    if f1_score>.68:\n",
    "                        print(\"*************************************\")\n",
    "                        print(\"N=\",N)\n",
    "                        print(\"k=\",k)\n",
    "                        print(\"randmx\",randmx)\n",
    "                        print(\"dist_percent\",dist_percent)\n",
    "                        print(\"f1\",f1_score)\n",
    "                        print(\"sensitivity\",sensitivity)\n",
    "                        print(\"specificity\",specificity)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"N=\",N)\n",
    "print(\"k=\",k)\n",
    "print(\"randmx\",randmx)\n",
    "print(\"dist_percent\",dist_percent)\n",
    "print(\"f1\",f1_score)\n",
    "print(\"sensitivity\",sensitivity)\n",
    "print(\"specificity\",specificity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "indept_study",
   "language": "python",
   "name": "indept_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
