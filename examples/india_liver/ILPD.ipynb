{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from augmentdata import data_augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cm_others(y_actual,y_predict):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm1 = confusion_matrix(y_actual,y_predict)\n",
    "#     print('Confusion Matrix : \\n', cm1)\n",
    "\n",
    "    total1=sum(sum(cm1))\n",
    "    #####from confusion matrix calculate accuracy\n",
    "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "#     print ('Accuracy : ', accuracy1)\n",
    "\n",
    "    tn=cm1[0,0]\n",
    "    fp=cm1[0,1]\n",
    "    fn=cm1[1,0]\n",
    "    tp=cm1[1,1]\n",
    "\n",
    "    sensitivity1 = tp/(tp+fn)\n",
    "#     print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "    specificity1 = tn/(tn+fp)\n",
    "#     print('Specificity : ', specificity1)\n",
    "    recall=sensitivity1\n",
    "    precision=tp/(tp+fp)\n",
    "\n",
    "#     print(\"Precision = \",precision)\n",
    "#     print(\"Recall = \",recall)\n",
    "\n",
    "    f1_score=2*(precision*recall)/(precision+recall)\n",
    "\n",
    "#     print(\"F1 score = \",f1_score)\n",
    "\n",
    "    return sensitivity1,specificity1,f1_score\n",
    "\n",
    "def create_model(weight_path,input_dim):\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    model=None\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=input_dim, \n",
    "                activation='relu')) \n",
    "\n",
    "    model.add(Dense(20, \n",
    "                activation='relu'))\n",
    "    model.add(Dense(20, \n",
    "                activation='relu'))\n",
    "\n",
    "    model.add(Dense(20, \n",
    "                activation='relu'))\n",
    "\n",
    "    model.add(Dense(20, \n",
    "                activation='relu'))\n",
    "    model.add(Dense(20, \n",
    "                activation='relu'))\n",
    "\n",
    "    model.add(Dense(2, \n",
    "                activation='softmax'))\n",
    "    opt=keras.optimizers.Adam(lr=.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model, callbacks_list\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attribute Information:\n",
    "\n",
    "1. Age Age of the patient\n",
    "2. Gender Gender of the patient\n",
    "3. TB Total Bilirubin\n",
    "4. DB Direct Bilirubin\n",
    "5. Alkphos Alkaline Phosphotase\n",
    "6. Sgpt Alamine Aminotransferase\n",
    "7. Sgot Aspartate Aminotransferase\n",
    "8. TP Total Protiens\n",
    "9. ALB Albumin\n",
    "10. A/G Ratio Albumin and Globulin Ratio\n",
    "11. Selector field used to split the data into two sets (labeled by the experts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"Age\",\"Gender\",\"TB\",\"DB\",\"AAP\",\"Sgpt\",\"Sgot\",\"TP\",\"ALB\",\"AG_Ratio\",\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"data/ilpd.csv\",names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df['Gender'] = df['Gender'].astype(str)\n",
    "df.Gender = label_encoder.fit_transform(df[\"Gender\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in df.iterrows():\n",
    "    if row[\"Class\"] == 1:\n",
    "        df.at[index,'Class'] = 0\n",
    "    elif row[\"Class\"] == 2:\n",
    "        df.at[index,'Class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(583, 11)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>TB</th>\n",
       "      <th>DB</th>\n",
       "      <th>AAP</th>\n",
       "      <th>Sgpt</th>\n",
       "      <th>Sgot</th>\n",
       "      <th>TP</th>\n",
       "      <th>ALB</th>\n",
       "      <th>AG_Ratio</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>187</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>10.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>699</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>490</td>\n",
       "      <td>60</td>\n",
       "      <td>68</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>182</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>195</td>\n",
       "      <td>27</td>\n",
       "      <td>59</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Gender    TB   DB  AAP  Sgpt  Sgot   TP  ALB  AG_Ratio  Class\n",
       "0   65       0   0.7  0.1  187    16    18  6.8  3.3      0.90      0\n",
       "1   62       1  10.9  5.5  699    64   100  7.5  3.2      0.74      0\n",
       "2   62       1   7.3  4.1  490    60    68  7.0  3.3      0.89      0\n",
       "3   58       1   1.0  0.4  182    14    20  6.8  3.4      1.00      0\n",
       "4   72       1   3.9  2.0  195    27    59  7.3  2.4      0.40      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    416\n",
       "1    167\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age           int64\n",
       "Gender        int64\n",
       "TB          float64\n",
       "DB          float64\n",
       "AAP           int64\n",
       "Sgpt          int64\n",
       "Sgot          int64\n",
       "TP          float64\n",
       "ALB         float64\n",
       "AG_Ratio    float64\n",
       "Class         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age         0\n",
       "Gender      0\n",
       "TB          0\n",
       "DB          0\n",
       "AAP         0\n",
       "Sgpt        0\n",
       "Sgot        0\n",
       "TP          0\n",
       "ALB         0\n",
       "AG_Ratio    4\n",
       "Class       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"AG_Ratio\"]=df[\"AG_Ratio\"].fillna(df[\"AG_Ratio\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age         0\n",
       "Gender      0\n",
       "TB          0\n",
       "DB          0\n",
       "AAP         0\n",
       "Sgpt        0\n",
       "Sgot        0\n",
       "TP          0\n",
       "ALB         0\n",
       "AG_Ratio    0\n",
       "Class       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here take 20% out for testing\n",
    "np.random.seed(41)\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "train = df[msk]\n",
    "test = df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "477\n",
      "(106, 11)\n",
      "(477, 11)\n"
     ]
    }
   ],
   "source": [
    "print(len(test))\n",
    "print(len(train))\n",
    "print(test.shape)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    74\n",
      "1    32\n",
      "Name: Class, dtype: int64\n",
      "0    342\n",
      "1    135\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test[\"Class\"].value_counts())\n",
    "print(train[\"Class\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Age', 'Gender', 'TB', 'DB', 'AAP', 'Sgpt', 'Sgot', 'TP', 'ALB',\n",
      "       'AG_Ratio', 'Class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "columns=df.columns\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with no augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_results={}\n",
    "adasyn_results={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(477, 10)\n",
      "Training MLP\n",
      "Time taken to train =  15\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used model and loaded weights from file\n",
      "Trying MLP\n",
      "106/106 [==============================] - 0s 342us/step\n",
      "Accuracy: 67.92\n",
      "RF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/gfxhome/asislam25/.conda/envs/venv_vasic_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB\n",
      "SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/gfxhome/asislam25/.conda/envs/venv_vasic_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "results[\"0\"]={}\n",
    "smote_results[\"0\"]={}\n",
    "adasyn_results[\"0\"]={}\n",
    "source=train[train.columns[:-1]]\n",
    "print(source.shape)\n",
    "\n",
    "\n",
    "weight_path=\"weights/wt1.hdf5\"\n",
    "model,callbacks_list=create_model(weight_path,source.shape[1])\n",
    "target = list(train[\"Class\"])\n",
    "target=pd.get_dummies(target)\n",
    "start=time.time()\n",
    "print(\"Training MLP\")\n",
    "history = model.fit(source.values, target,epochs=200,validation_split=0.2,callbacks=callbacks_list,verbose=0)\n",
    "end=time.time()\n",
    "difference = int(end - start)\n",
    "print(\"Time taken to train = \",difference)\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# load weights\n",
    "model.load_weights(weight_path)\n",
    "# Compile model (required to make predictions)\n",
    "opt=keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "print(\"used model and loaded weights from file\")\n",
    "\n",
    "print(\"Trying MLP\")\n",
    "results[\"0\"][\"MLP\"]={}\n",
    "smote_results[\"0\"][\"MLP\"]={}\n",
    "adasyn_results[\"0\"][\"MLP\"]={}\n",
    "\n",
    "y_actual = list(test[\"Class\"])\n",
    "y_actual=pd.get_dummies(y_actual)\n",
    "test_features_only=test[test.columns[:-1]]\n",
    "_, accuracy = model.evaluate(test_features_only.values, y_actual)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "y_actual = test[\"Class\"].astype(int)\n",
    "y_actual=np.asarray(y_actual)\n",
    "y_predict=model.predict_classes(test_features_only.values)\n",
    "sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "results[\"0\"][\"MLP\"][\"F1\"]=f1_score\n",
    "results[\"0\"][\"MLP\"][\"sensitivity\"]=sensitivity\n",
    "results[\"0\"][\"MLP\"][\"specificity\"]=specificity\n",
    "\n",
    "smote_results[\"0\"][\"MLP\"][\"F1\"]=f1_score  \n",
    "smote_results[\"0\"][\"MLP\"][\"sensitivity\"]=sensitivity\n",
    "smote_results[\"0\"][\"MLP\"][\"specificity\"]=specificity\n",
    "\n",
    "adasyn_results[\"0\"][\"MLP\"][\"F1\"]=f1_score  \n",
    "adasyn_results[\"0\"][\"MLP\"][\"sensitivity\"]=sensitivity\n",
    "adasyn_results[\"0\"][\"MLP\"][\"specificity\"]=specificity\n",
    "\n",
    "\n",
    "# for other classifiers\n",
    "target = list(train[\"Class\"])\n",
    "\n",
    "# Random forest\n",
    "print(\"RF\")\n",
    "results[\"0\"][\"RF\"]={}\n",
    "smote_results[\"0\"][\"RF\"]={}\n",
    "adasyn_results[\"0\"][\"RF\"]={}\n",
    "\n",
    "clf=RandomForestClassifier()\n",
    "clf.fit(source.values,target)\n",
    "y_pred=None\n",
    "y_predict=clf.predict(test_features_only.values)\n",
    "sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "results[\"0\"][\"RF\"][\"F1\"]=f1_score\n",
    "results[\"0\"][\"RF\"][\"sensitivity\"]=sensitivity\n",
    "results[\"0\"][\"RF\"][\"specificity\"]=specificity\n",
    "\n",
    "smote_results[\"0\"][\"RF\"][\"F1\"]=f1_score  \n",
    "smote_results[\"0\"][\"RF\"][\"sensitivity\"]=sensitivity\n",
    "smote_results[\"0\"][\"RF\"][\"specificity\"]=specificity\n",
    "\n",
    "adasyn_results[\"0\"][\"RF\"][\"F1\"]=f1_score  \n",
    "adasyn_results[\"0\"][\"RF\"][\"sensitivity\"]=sensitivity\n",
    "adasyn_results[\"0\"][\"RF\"][\"specificity\"]=specificity\n",
    "\n",
    "#     Gaussian NB\n",
    "gnb = GaussianNB()\n",
    "results[\"0\"][\"GNB\"]={}\n",
    "smote_results[\"0\"][\"GNB\"]={}\n",
    "adasyn_results[\"0\"][\"GNB\"]={}\n",
    "\n",
    "print(\"GNB\")\n",
    "gnb.fit(source.values,target)\n",
    "y_predict = gnb.predict(test_features_only.values)\n",
    "sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "results[\"0\"][\"GNB\"][\"F1\"]=f1_score\n",
    "results[\"0\"][\"GNB\"][\"sensitivity\"]=sensitivity\n",
    "results[\"0\"][\"GNB\"][\"specificity\"]=specificity\n",
    "\n",
    "smote_results[\"0\"][\"GNB\"][\"F1\"]=f1_score  \n",
    "smote_results[\"0\"][\"GNB\"][\"sensitivity\"]=sensitivity\n",
    "smote_results[\"0\"][\"GNB\"][\"specificity\"]=specificity\n",
    "\n",
    "adasyn_results[\"0\"][\"GNB\"][\"F1\"]=f1_score  \n",
    "adasyn_results[\"0\"][\"GNB\"][\"sensitivity\"]=sensitivity\n",
    "adasyn_results[\"0\"][\"GNB\"][\"specificity\"]=specificity\n",
    "\n",
    "#     SVM\n",
    "clf = svm.SVC()\n",
    "results[\"0\"][\"SVM\"]={}\n",
    "smote_results[\"0\"][\"SVM\"]={}\n",
    "adasyn_results[\"0\"][\"SVM\"]={}\n",
    "\n",
    "print(\"SVM\")\n",
    "clf.fit(source.values,target)\n",
    "y_predict=clf.predict(test_features_only.values)\n",
    "sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "results[\"0\"][\"SVM\"][\"F1\"]=f1_score\n",
    "results[\"0\"][\"SVM\"][\"sensitivity\"]=sensitivity\n",
    "results[\"0\"][\"SVM\"][\"specificity\"]=specificity\n",
    "\n",
    "smote_results[\"0\"][\"SVM\"][\"F1\"]=f1_score  \n",
    "smote_results[\"0\"][\"SVM\"][\"sensitivity\"]=sensitivity\n",
    "smote_results[\"0\"][\"SVM\"][\"specificity\"]=specificity\n",
    "\n",
    "adasyn_results[\"0\"][\"SVM\"][\"F1\"]=f1_score  \n",
    "adasyn_results[\"0\"][\"SVM\"][\"sensitivity\"]=sensitivity\n",
    "adasyn_results[\"0\"][\"SVM\"][\"specificity\"]=specificity\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'MLP': {'F1': nan,\n",
       "   'sensitivity': 0.0,\n",
       "   'specificity': 0.972972972972973},\n",
       "  'RF': {'F1': 0.49056603773584906,\n",
       "   'sensitivity': 0.40625,\n",
       "   'specificity': 0.8918918918918919},\n",
       "  'GNB': {'F1': 0.6138613861386139,\n",
       "   'sensitivity': 0.96875,\n",
       "   'specificity': 0.4864864864864865},\n",
       "  'SVM': {'F1': nan, 'sensitivity': 0.0, 'specificity': 1.0}}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'MLP': {'F1': nan,\n",
       "   'sensitivity': 0.0,\n",
       "   'specificity': 0.972972972972973},\n",
       "  'RF': {'F1': 0.49056603773584906,\n",
       "   'sensitivity': 0.40625,\n",
       "   'specificity': 0.8918918918918919},\n",
       "  'GNB': {'F1': 0.6138613861386139,\n",
       "   'sensitivity': 0.96875,\n",
       "   'specificity': 0.4864864864864865},\n",
       "  'SVM': {'F1': nan, 'sensitivity': 0.0, 'specificity': 1.0}}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smote_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'MLP': {'F1': nan,\n",
       "   'sensitivity': 0.0,\n",
       "   'specificity': 0.972972972972973},\n",
       "  'RF': {'F1': 0.49056603773584906,\n",
       "   'sensitivity': 0.40625,\n",
       "   'specificity': 0.8918918918918919},\n",
       "  'GNB': {'F1': 0.6138613861386139,\n",
       "   'sensitivity': 0.96875,\n",
       "   'specificity': 0.4864864864864865},\n",
       "  'SVM': {'F1': nan, 'sensitivity': 0.0, 'specificity': 1.0}}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adasyn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    342\n",
       "1    135\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train  with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_range=[50,100,198]\n",
    "for N in N_range:\n",
    "    results[N]={}\n",
    "    smote_results[N]={}\n",
    "    adasyn_results[N]={}\n",
    "    \n",
    "    k_range=[1,2,5,10]\n",
    "    for k in k_range:\n",
    "        results[N][k]={}\n",
    "        smote_results[N][k]={}  \n",
    "        adasyn_results[N][k]={}\n",
    "        \n",
    "        results[N][k][\"MLP\"]={}\n",
    "        smote_results[N][k][\"MLP\"]={}  \n",
    "        adasyn_results[N][k][\"MLP\"]={}\n",
    "\n",
    "        results[N][k][\"RF\"]={}\n",
    "        smote_results[N][k][\"RF\"]={}  \n",
    "        adasyn_results[N][k][\"RF\"]={}\n",
    "        \n",
    "        results[N][k][\"GNB\"]={}\n",
    "        smote_results[N][k][\"GNB\"]={}  \n",
    "        adasyn_results[N][k][\"GNB\"]={}\n",
    "\n",
    "        results[N][k][\"SVM\"]={}\n",
    "        smote_results[N][k][\"SVM\"]={}  \n",
    "        adasyn_results[N][k][\"SVM\"]={}\n",
    "\n",
    "\n",
    "        class_index=1\n",
    "        randmx=.001\n",
    "        dist_percent=0.2\n",
    "\n",
    "        \n",
    "        daug = data_augment.DataAugment()\n",
    "        print(\"randmx = \",randmx)\n",
    "        now = time.time()\n",
    "        [Data_a,Ext_d,Ext_not]=daug.augment(data=train.values,k=k,class_ind=class_index,N=N,\n",
    "                                            randmx=randmx,dist_percent=dist_percent)\n",
    "        later = time.time()\n",
    "        difference = int(later - now)\n",
    "        print(\"Time taken to augment = \",difference)\n",
    "        print(len(Data_a))\n",
    "\n",
    "        train_aug=pd.DataFrame(data=Data_a,index=None,    # values                \n",
    "                columns=columns)      \n",
    "\n",
    "        print(\"After augmentation of \",N,\" items with \",k,\" neighbors\")\n",
    "\n",
    "        print(train_aug[\"Class\"].value_counts())\n",
    "        source=train_aug[train_aug.columns[:-1]]\n",
    "        target = list(train_aug[\"Class\"])\n",
    "        target=pd.get_dummies(target)\n",
    "        \n",
    "#         # going for DNN\n",
    "\n",
    "#         weight_path=\"weights/\"+str(N)+\"_\"+str(k)+\"_wt1.hdf5\"    \n",
    "#         model,callbacks_list=create_model(weight_path,source.shape[1])\n",
    "#         start=time.time()\n",
    "#         print(\"Training model for N = \",N,\" k = \",k)\n",
    "#         history = model.fit(source.values, target,epochs=200,validation_split=0.2,callbacks=callbacks_list,verbose=0)\n",
    "#         end=time.time()\n",
    "#         difference = int(end - start)\n",
    "#         print(\"Time taken to train = \",difference)\n",
    "\n",
    "\n",
    "\n",
    "#         plt.plot(history.history['loss'])\n",
    "#         plt.plot(history.history['val_loss'])\n",
    "#         plt.title('model loss')\n",
    "#         plt.ylabel('loss')\n",
    "#         plt.xlabel('epoch')\n",
    "#         plt.legend(['train','validation'], loc='upper left')\n",
    "#         plt.show()    \n",
    "\n",
    "#         # load weights\n",
    "#         model.load_weights(weight_path)\n",
    "#         # Compile model (required to make predictions)\n",
    "#         opt=keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "#         model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "#         print(\"used model and loaded weights from file\")    \n",
    "#         print(\"Test distribution\")\n",
    "#         print(test[\"Class\"].value_counts())\n",
    "#         print(\"Trying MLP\")\n",
    "#         y_actual = list(test[\"Class\"])\n",
    "#         y_actual=pd.get_dummies(y_actual)\n",
    "#         test_features_only=test[test.columns[:-1]]\n",
    "#         _, accuracy = model.evaluate(test_features_only.values, y_actual)\n",
    "#         print('Accuracy: %.2f' % (accuracy*100))            \n",
    "\n",
    "\n",
    "#         y_actual = test[\"Class\"].astype(int)\n",
    "#         y_actual=np.asarray(y_actual)\n",
    "#         y_predict=model.predict_classes(test_features_only.values)\n",
    "#         sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "#         results[N][k][\"MLP\"][\"F1\"]=f1_score \n",
    "#         results[N][k][\"MLP\"][\"sensitivity\"]=sensitivity \n",
    "#         results[N][k][\"MLP\"][\"specificity\"]=specificity         \n",
    "        \n",
    "\n",
    "\n",
    "        # for other classifiers\n",
    "        target = list(train_aug[\"Class\"])\n",
    "        source=train_aug[train_aug.columns[:-1]]\n",
    "        test_features_only=test[test.columns[:-1]]\n",
    "\n",
    "        # Random forest\n",
    "#         print(\"RF\")\n",
    "#         clf=RandomForestClassifier()\n",
    "#         clf.fit(source.values,target)\n",
    "#         y_predict=clf.predict(test_features_only.values)\n",
    "#         sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "#         results[N][k][\"RF\"][\"F1\"]=f1_score \n",
    "#         results[N][k][\"RF\"][\"sensitivity\"]=sensitivity \n",
    "#         results[N][k][\"RF\"][\"specificity\"]=specificity         \n",
    "                \n",
    "\n",
    "    #     Gaussian NB\n",
    "        print(\"GNB\")\n",
    "        gnb = GaussianNB()\n",
    "        gnb.fit(source.values,target)\n",
    "        y_predict = gnb.predict(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        results[N][k][\"GNB\"][\"F1\"]=f1_score \n",
    "        results[N][k][\"GNB\"][\"sensitivity\"]=sensitivity \n",
    "        results[N][k][\"GNB\"][\"specificity\"]=specificity         \n",
    "                        \n",
    "        \n",
    "\n",
    "    #     SVM\n",
    "#         print(\"SVM\")\n",
    "#         clf = svm.SVC()\n",
    "#         clf.fit(source.values,target)\n",
    "#         y_predict=clf.predict(test_features_only.values)\n",
    "#         sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "#         results[N][k][\"SVM\"][\"F1\"]=f1_score \n",
    "#         results[N][k][\"SVM\"][\"sensitivity\"]=sensitivity \n",
    "#         results[N][k][\"SVM\"][\"specificity\"]=specificity         \n",
    "                                \n",
    "        \n",
    "        \n",
    "        \n",
    "        # this part for SMOTE\n",
    "        print(\"******Starting SMOTE***********\")\n",
    "        source=train[train.columns[:-1]]\n",
    "        print(\"Shape of training \",source.shape)\n",
    "\n",
    "        y_train=train[\"Class\"].values\n",
    "        num_minority=sum(y_train==class_index)\n",
    "        num_majority=len(y_train)-num_minority\n",
    "        print(num_majority,num_minority)\n",
    "        \n",
    "        samp_strategy=(num_minority+N)/num_majority\n",
    "        print(samp_strategy)\n",
    "        now = time.time()\n",
    "        sm = SMOTE(random_state=2,k_neighbors=k,sampling_strategy=samp_strategy)\n",
    "        X_train_res, y_train_res = sm.fit_sample(source, y_train.ravel())\n",
    "        source=X_train_res\n",
    "\n",
    "        later = time.time()\n",
    "        difference = int(later - now)\n",
    "        print(\"Time taken to augment by SMOTE = \",difference)\n",
    "\n",
    "        print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
    "        print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
    "\n",
    "        print(\"After OverSampling, counts of label {}: {}\".format(class_index,sum(y_train_res==1)))\n",
    "        print(\"After OverSampling, counts of other label: {}\".format(len(y_train_res)-sum(y_train_res==class_index)))  \n",
    "        \n",
    "        # starting dnn\n",
    "#         weight_path=\"weights/smote\"+\"_\"+str(N)+\"_\"+str(k)+\"_wt1.hdf5\"    \n",
    "#         model,callbacks_list=create_model(weight_path,X_train_res.shape[1])\n",
    "#         start=time.time()\n",
    "#         print(\"Training model\")\n",
    "#         target=pd.get_dummies(y_train_res)\n",
    "#         history = model.fit(X_train_res.values, target,epochs=200,validation_split=0.2,callbacks=callbacks_list,verbose=0)\n",
    "#         end=time.time()\n",
    "#         difference = int(end - start)\n",
    "#         print(\"Time taken to train = \",difference) \n",
    "\n",
    "#         plt.plot(history.history['loss'])\n",
    "#         plt.plot(history.history['val_loss'])\n",
    "#         plt.title('model loss')\n",
    "#         plt.ylabel('loss')\n",
    "#         plt.xlabel('epoch')\n",
    "#         plt.legend(['train','validation'], loc='upper left')\n",
    "#         plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "#         # load weights\n",
    "#         model.load_weights(weight_path)\n",
    "#         # Compile model (required to make predictions)\n",
    "#         opt=keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "#         model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "#         print(\"used model and loaded weights from file\")    \n",
    "#         print(\"Trying MLP\")\n",
    "#         y_actual = list(test[\"Class\"])\n",
    "#         y_actual=pd.get_dummies(y_actual)\n",
    "#         test_features_only=test[test.columns[:-1]]\n",
    "#         _, accuracy = model.evaluate(test_features_only.values, y_actual)\n",
    "#         print('Accuracy: %.2f' % (accuracy*100)) \n",
    "\n",
    "\n",
    "\n",
    "#         y_actual = test[\"Class\"].astype(int)\n",
    "#         y_actual=np.asarray(y_actual)\n",
    "#         y_predict=model.predict_classes(test_features_only.values)\n",
    "#         sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "#         smote_results[N][k][\"MLP\"][\"F1\"]=f1_score         \n",
    "#         smote_results[N][k][\"MLP\"][\"sensitivity\"]=sensitivity                 \n",
    "#         smote_results[N][k][\"MLP\"][\"specificity\"]=specificity                 \n",
    "\n",
    "        # for other classifiers\n",
    "        target = y_train_res\n",
    "        source=X_train_res\n",
    "        test_features_only=test[test.columns[:-1]]\n",
    "\n",
    "        # Random forest\n",
    "#         print(\"RF\")\n",
    "#         clf=RandomForestClassifier()\n",
    "#         clf.fit(source.values,target)\n",
    "#         y_predict=clf.predict(test_features_only.values)\n",
    "#         sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "#         smote_results[N][k][\"RF\"][\"F1\"]=f1_score         \n",
    "#         smote_results[N][k][\"RF\"][\"sensitivity\"]=sensitivity                 \n",
    "#         smote_results[N][k][\"RF\"][\"specificity\"]=specificity\n",
    "\n",
    "    #     Gaussian NB\n",
    "        print(\"GNB\")\n",
    "        gnb = GaussianNB()\n",
    "        gnb.fit(source.values,target)\n",
    "        y_predict = gnb.predict(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        smote_results[N][k][\"GNB\"][\"F1\"]=f1_score         \n",
    "        smote_results[N][k][\"GNB\"][\"sensitivity\"]=sensitivity                 \n",
    "        smote_results[N][k][\"GNB\"][\"specificity\"]=specificity\n",
    "\n",
    "    #     SVM\n",
    "#         print(\"SVM\")\n",
    "#         clf = svm.SVC()\n",
    "#         clf.fit(source.values,target)\n",
    "#         y_predict=clf.predict(test_features_only.values)\n",
    "#         sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "#         smote_results[N][k][\"SVM\"][\"F1\"]=f1_score         \n",
    "#         smote_results[N][k][\"SVM\"][\"sensitivity\"]=sensitivity                 \n",
    "#         smote_results[N][k][\"SVM\"][\"specificity\"]=specificity\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # this part for ADASYN\n",
    "        print(\"******Starting ADASYN***********\")\n",
    "        source=train[train.columns[:-1]]\n",
    "        print(\"Shape of training \",source.shape)\n",
    "\n",
    "        y_train=train[\"Class\"].values\n",
    "        num_minority=sum(y_train==class_index)\n",
    "        num_majority=len(y_train)-num_minority\n",
    "        print(num_majority,num_minority)\n",
    "        \n",
    "        samp_strategy=(num_minority+N)/num_majority\n",
    "        print(samp_strategy)\n",
    "        now = time.time()\n",
    "        ad = ADASYN(random_state=2,n_neighbors=k,sampling_strategy=samp_strategy)\n",
    "        X_train_res, y_train_res = ad.fit_sample(source, y_train.ravel())\n",
    "        source=X_train_res\n",
    "\n",
    "        later = time.time()\n",
    "        difference = int(later - now)\n",
    "        print(\"Time taken to augment by ADASYN = \",difference)\n",
    "\n",
    "        print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
    "        print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
    "\n",
    "        print(\"After OverSampling, counts of label {}: {}\".format(class_index,sum(y_train_res==1)))\n",
    "        print(\"After OverSampling, counts of other label: {}\".format(len(y_train_res)-sum(y_train_res==class_index)))  \n",
    "        \n",
    "        weight_path=\"weights/adasyn\"+\"_\"+str(N)+\"_\"+str(k)+\"_wt1.hdf5\"    \n",
    "        model,callbacks_list=create_model(weight_path,X_train_res.shape[1])\n",
    "        start=time.time()\n",
    "        print(\"Training model\")\n",
    "        target=pd.get_dummies(y_train_res)\n",
    "        history = model.fit(X_train_res.values, target,epochs=200,validation_split=0.2,callbacks=callbacks_list,verbose=0)\n",
    "        end=time.time()\n",
    "        difference = int(end - start)\n",
    "        print(\"Time taken to train = \",difference) \n",
    "\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train','validation'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "        # load weights\n",
    "        model.load_weights(weight_path)\n",
    "        # Compile model (required to make predictions)\n",
    "        opt=keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "        print(\"used model and loaded weights from file\")    \n",
    "        print(\"Trying MLP\")\n",
    "        y_actual = list(test[\"Class\"])\n",
    "        y_actual=pd.get_dummies(y_actual)\n",
    "        test_features_only=test[test.columns[:-1]]\n",
    "        _, accuracy = model.evaluate(test_features_only.values, y_actual)\n",
    "        print('Accuracy: %.2f' % (accuracy*100)) \n",
    "\n",
    "\n",
    "\n",
    "        y_actual = test[\"Class\"].astype(int)\n",
    "        y_actual=np.asarray(y_actual)\n",
    "        y_predict=model.predict_classes(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        adasyn_results[N][k][\"MLP\"][\"F1\"]=f1_score         \n",
    "        adasyn_results[N][k][\"MLP\"][\"sensitivity\"]=sensitivity                 \n",
    "        adasyn_results[N][k][\"MLP\"][\"specificity\"]=specificity\n",
    "\n",
    "        # for other classifiers\n",
    "        target = y_train_res\n",
    "        source=X_train_res\n",
    "        test_features_only=test[test.columns[:-1]]\n",
    "\n",
    "        # Random forest\n",
    "        print(\"RF\")\n",
    "        clf=RandomForestClassifier()\n",
    "        clf.fit(source.values,target)\n",
    "        y_predict=clf.predict(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        adasyn_results[N][k][\"RF\"][\"F1\"]=f1_score         \n",
    "        adasyn_results[N][k][\"RF\"][\"sensitivity\"]=sensitivity                 \n",
    "        adasyn_results[N][k][\"RF\"][\"specificity\"]=specificity\n",
    "\n",
    "    #     Gaussian NB\n",
    "        print(\"GNB\")\n",
    "        gnb = GaussianNB()\n",
    "        gnb.fit(source.values,target)\n",
    "        y_predict = gnb.predict(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        adasyn_results[N][k][\"GNB\"][\"F1\"]=f1_score         \n",
    "        adasyn_results[N][k][\"GNB\"][\"sensitivity\"]=sensitivity                 \n",
    "        adasyn_results[N][k][\"GNB\"][\"specificity\"]=specificity\n",
    "\n",
    "    #     SVM\n",
    "        print(\"SVM\")\n",
    "        clf = svm.SVC()\n",
    "        clf.fit(source.values,target)\n",
    "        y_predict=clf.predict(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        adasyn_results[N][k][\"SVM\"][\"F1\"]=f1_score         \n",
    "        adasyn_results[N][k][\"SVM\"][\"sensitivity\"]=sensitivity                 \n",
    "        adasyn_results[N][k][\"SVM\"][\"specificity\"]=specificity        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best\n",
    "\n",
    "print(\"randmx = \",randmx)\n",
    "print(\"dist_percent = \",dist_percent)\n",
    "import math\n",
    "checks=[\"MLP\",\"RF\",\"GNB\",\"SVM\"]\n",
    "# checks=[\"GNB\"]\n",
    "\n",
    "best_n=None\n",
    "best_k=None\n",
    "best_sensitivity=0\n",
    "best_specificity=0\n",
    "best_f1=0\n",
    "\n",
    "print(\"KNNOR\")\n",
    "\n",
    "\n",
    "for check in checks:\n",
    "    for k,v in results.items():\n",
    "        if k==\"0\":\n",
    "            continue\n",
    "        for k1,v1 in v.items():\n",
    "#             print(k1)\n",
    "#             print(v1)\n",
    "            if math.isnan(v1[check][\"F1\"]):\n",
    "                continue\n",
    "#             print(v1[check][\"F1\"])\n",
    "            if best_f1<v1[check][\"F1\"]:\n",
    "                best_f1=v1[check][\"F1\"]\n",
    "                best_n=k\n",
    "                best_k=k1\n",
    "                best_sensitivity=v1[check][\"sensitivity\"]\n",
    "                best_specificity=v1[check][\"specificity\"]\n",
    "                \n",
    "\n",
    "    print(check,best_f1,\"[N=\",best_n,\",k=\",best_k,\"]\")\n",
    "    print(\"Sensitivity = \",best_sensitivity)\n",
    "    print(\"Specificity = \",best_specificity)    \n",
    "    \n",
    "    best_f1=0\n",
    "    best_k=None\n",
    "    best_n=None\n",
    "    best_sensitivity=0\n",
    "    best_specificity=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best\n",
    "\n",
    "print(\"randmx = \",randmx)\n",
    "print(\"dist_percent = \",dist_percent)\n",
    "import math\n",
    "checks=[\"MLP\",\"RF\",\"GNB\",\"SVM\"]\n",
    "# checks=[\"GNB\"]\n",
    "\n",
    "best_n=None\n",
    "best_k=None\n",
    "best_sensitivity=0\n",
    "best_specificity=0\n",
    "best_f1=0\n",
    "\n",
    "print(\"SMOTE\")\n",
    "\n",
    "\n",
    "for check in checks:\n",
    "    for k,v in smote_results.items():\n",
    "        if k==\"0\":\n",
    "            continue\n",
    "        for k1,v1 in v.items():\n",
    "#             print(k1)\n",
    "#             print(v1)\n",
    "            if math.isnan(v1[check][\"F1\"]):\n",
    "                continue\n",
    "#             print(v1[check][\"F1\"])\n",
    "            if best_f1<v1[check][\"F1\"]:\n",
    "                best_f1=v1[check][\"F1\"]\n",
    "                best_n=k\n",
    "                best_k=k1\n",
    "                best_sensitivity=v1[check][\"sensitivity\"]\n",
    "                best_specificity=v1[check][\"specificity\"]                \n",
    "\n",
    "    print(check,best_f1,\"[N=\",best_n,\",k=\",best_k,\"]\")\n",
    "    print(\"Sensitivity = \",best_sensitivity)\n",
    "    print(\"Specificity = \",best_specificity)\n",
    "    \n",
    "    best_f1=0\n",
    "    best_k=None\n",
    "    best_n=None\n",
    "    best_sensitivity=0\n",
    "    best_specificity=0\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best\n",
    "\n",
    "print(\"randmx = \",randmx)\n",
    "print(\"dist_percent = \",dist_percent)\n",
    "import math\n",
    "checks=[\"MLP\",\"RF\",\"GNB\",\"SVM\"]\n",
    "# checks=[\"GNB\"]\n",
    "\n",
    "best_n=None\n",
    "best_k=None\n",
    "best_sensitivity=0\n",
    "best_specificity=0\n",
    "best_f1=0\n",
    "\n",
    "print(\"ADASYN\")\n",
    "\n",
    "\n",
    "for check in checks:\n",
    "    for k,v in adasyn_results.items():\n",
    "        if k==\"0\":\n",
    "            continue\n",
    "        for k1,v1 in v.items():\n",
    "#             print(k1)\n",
    "#             print(v1)\n",
    "            if math.isnan(v1[check][\"F1\"]):\n",
    "                continue\n",
    "#             print(v1[check][\"F1\"])\n",
    "            if best_f1<v1[check][\"F1\"]:\n",
    "                best_f1=v1[check][\"F1\"]\n",
    "                best_n=k\n",
    "                best_k=k1\n",
    "                best_sensitivity=v1[check][\"sensitivity\"]\n",
    "                best_specificity=v1[check][\"specificity\"]                \n",
    "\n",
    "    print(check,best_f1,\"[N=\",best_n,\",k=\",best_k,\"]\")\n",
    "    print(\"Sensitivity = \",best_sensitivity)\n",
    "    print(\"Specificity = \",best_specificity)\n",
    "    \n",
    "    best_f1=0\n",
    "    best_k=None\n",
    "    best_n=None\n",
    "    best_sensitivity=0\n",
    "    best_specificity=0\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adasyn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below for re training as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-a055e25687a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 [Data_a,Ext_d,Ext_not]=daug.augment(data=train.values,k=k,class_ind=class_index,N=N,\n\u001b[0;32m---> 49\u001b[0;31m                                                     randmx=randmx,dist_percent=dist_percent)\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mlater\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mdifference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlater\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/hbku/spring_2020/independednt_Study_Prof_Samir/venv_indept_study/lib/python3.6/site-packages/augmentdata/data_augment.py\u001b[0m in \u001b[0;36maugment\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    249\u001b[0m                                                 \u001b[0;31m# for testing the artificial data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                                                 \u001b[0mcan_use\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neighbors_to_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m                                                 \u001b[0;31m# print(\"Usability \",can_use)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/hbku/spring_2020/independednt_Study_Prof_Samir/venv_indept_study/lib/python3.6/site-packages/augmentdata/test_nn.py\u001b[0m in \u001b[0;36mpredict_classification\u001b[0;34m(train, test_row, num_neighbors, test_val)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Make a classification prediction with neighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neighbors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mneighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moutput_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# print(output_values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/hbku/spring_2020/independednt_Study_Prof_Samir/venv_indept_study/lib/python3.6/site-packages/augmentdata/test_nn.py\u001b[0m in \u001b[0;36mget_neighbors\u001b[0;34m(train, test_row, num_neighbors)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_row\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuclidean_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/hbku/spring_2020/independednt_Study_Prof_Samir/venv_indept_study/lib/python3.6/site-packages/augmentdata/test_nn.py\u001b[0m in \u001b[0;36meuclidean_distance\u001b[0;34m(row1, row2)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0mdistance\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrow2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results={}\n",
    "smote_results={}\n",
    "adasyn_results={}\n",
    "\n",
    "# N_range=[50,100,198]\n",
    "while True:\n",
    "    N_range=[100]\n",
    "    for N in N_range:\n",
    "        results[N]={}\n",
    "        smote_results[N]={}\n",
    "        adasyn_results[N]={}\n",
    "\n",
    "        k_range=[5,10]\n",
    "#         k_range=[2]\n",
    "        for k in k_range:\n",
    "            randmx_range=[0.01,0.05,0.1,0.5,0.9]\n",
    "        \n",
    "            for randmx in randmx_range:\n",
    "                results[N][k]={}\n",
    "                smote_results[N][k]={}  \n",
    "                adasyn_results[N][k]={}\n",
    "\n",
    "                results[N][k][\"MLP\"]={}\n",
    "                smote_results[N][k][\"MLP\"]={}  \n",
    "                adasyn_results[N][k][\"MLP\"]={}\n",
    "\n",
    "                results[N][k][\"RF\"]={}\n",
    "                smote_results[N][k][\"RF\"]={}  \n",
    "                adasyn_results[N][k][\"RF\"]={}\n",
    "\n",
    "                results[N][k][\"GNB\"]={}\n",
    "                smote_results[N][k][\"GNB\"]={}  \n",
    "                adasyn_results[N][k][\"GNB\"]={}\n",
    "\n",
    "                results[N][k][\"SVM\"]={}\n",
    "                smote_results[N][k][\"SVM\"]={}  \n",
    "                adasyn_results[N][k][\"SVM\"]={}\n",
    "\n",
    "\n",
    "                class_index=1\n",
    "                randmx=1\n",
    "                dist_percent=0.9\n",
    "\n",
    "\n",
    "                daug = data_augment.DataAugment()\n",
    "    #             print(\"randmx = \",randmx)\n",
    "                now = time.time()\n",
    "                [Data_a,Ext_d,Ext_not]=daug.augment(data=train.values,k=k,class_ind=class_index,N=N,\n",
    "                                                    randmx=randmx,dist_percent=dist_percent)\n",
    "                later = time.time()\n",
    "                difference = int(later - now)\n",
    "    #             print(\"Time taken to augment = \",difference)\n",
    "    #             print(len(Data_a))\n",
    "\n",
    "                train_aug=pd.DataFrame(data=Data_a,index=None,    # values                \n",
    "                        columns=columns)      \n",
    "\n",
    "    #             print(\"After augmentation of \",N,\" items with \",k,\" neighbors\")\n",
    "\n",
    "    #             print(train_aug[\"Class\"].value_counts())\n",
    "                source=train_aug[train_aug.columns[:-1]]\n",
    "                target = list(train_aug[\"Class\"])\n",
    "                target=pd.get_dummies(target)\n",
    "\n",
    "    #             weight_path=\"weights/\"+str(N)+\"_\"+str(k)+\"_wt1.hdf5\"    \n",
    "    #             model,callbacks_list=create_model(weight_path,source.shape[1])\n",
    "    #             start=time.time()\n",
    "    #             print(\"Training model for N = \",N,\" k = \",k)\n",
    "    #             history = model.fit(source.values, target,epochs=200,validation_split=0.2,callbacks=callbacks_list,verbose=0)\n",
    "    #             end=time.time()\n",
    "    #             difference = int(end - start)\n",
    "    #             print(\"Time taken to train = \",difference)\n",
    "\n",
    "\n",
    "\n",
    "    #             plt.plot(history.history['loss'])\n",
    "    #             plt.plot(history.history['val_loss'])\n",
    "    #             plt.title('model loss')\n",
    "    #             plt.ylabel('loss')\n",
    "    #             plt.xlabel('epoch')\n",
    "    #             plt.legend(['train','validation'], loc='upper left')\n",
    "    #             plt.show()    \n",
    "\n",
    "    #             # load weights\n",
    "    #             model.load_weights(weight_path)\n",
    "    #             # Compile model (required to make predictions)\n",
    "    #             opt=keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    #             model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    #             print(\"used model and loaded weights from file\")    \n",
    "    #             print(\"Test distribution\")\n",
    "    #             print(test[\"Class\"].value_counts())\n",
    "    #             print(\"Trying MLP\")\n",
    "    #             y_actual = list(test[\"Class\"])\n",
    "    #             y_actual=pd.get_dummies(y_actual)\n",
    "    #             test_features_only=test[test.columns[:-1]]\n",
    "    #             _, accuracy = model.evaluate(test_features_only.values, y_actual)\n",
    "    #             print('Accuracy: %.2f' % (accuracy*100))            \n",
    "\n",
    "\n",
    "                y_actual = test[\"Class\"].astype(int)\n",
    "                y_actual=np.asarray(y_actual)\n",
    "    #             y_predict=model.predict_classes(test_features_only.values)\n",
    "    #             sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "    #             results[N][k][\"MLP\"][\"F1\"]=f1_score \n",
    "    #             results[N][k][\"MLP\"][\"sensitivity\"]=sensitivity \n",
    "    #             results[N][k][\"MLP\"][\"specificity\"]=specificity         \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # for other classifiers\n",
    "                target = list(train_aug[\"Class\"])\n",
    "                source=train_aug[train_aug.columns[:-1]]\n",
    "                test_features_only=test[test.columns[:-1]]\n",
    "\n",
    "    #             # Random forest\n",
    "    #             print(\"RF\")\n",
    "    #             clf=RandomForestClassifier()\n",
    "    #             clf.fit(source.values,target)\n",
    "    #             y_predict=clf.predict(test_features_only.values)\n",
    "    #             sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "    #             results[N][k][\"RF\"][\"F1\"]=f1_score \n",
    "    #             results[N][k][\"RF\"][\"sensitivity\"]=sensitivity \n",
    "    #             results[N][k][\"RF\"][\"specificity\"]=specificity         \n",
    "\n",
    "\n",
    "            #     Gaussian NB\n",
    "#                 print(\"GNB\")\n",
    "                gnb = GaussianNB()\n",
    "                gnb.fit(source.values,target)\n",
    "                y_predict = gnb.predict(test_features_only.values)\n",
    "                sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "                results[N][k][\"GNB\"][\"F1\"]=f1_score \n",
    "                results[N][k][\"GNB\"][\"sensitivity\"]=sensitivity \n",
    "                results[N][k][\"GNB\"][\"specificity\"]=specificity         \n",
    "\n",
    "\n",
    "\n",
    "            #     SVM\n",
    "    #             print(\"SVM\")\n",
    "    #             clf = svm.SVC()\n",
    "    #             clf.fit(source.values,target)\n",
    "    #             y_predict=clf.predict(test_features_only.values)\n",
    "    #             sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "    #             results[N][k][\"SVM\"][\"F1\"]=f1_score \n",
    "    #             results[N][k][\"SVM\"][\"sensitivity\"]=sensitivity \n",
    "    #             results[N][k][\"SVM\"][\"specificity\"]=specificity         \n",
    "\n",
    "#                 print(\"*************************************\")\n",
    "                \n",
    "                target_f1=62.5\n",
    "                target_sensitivity=96.8\n",
    "\n",
    "                if sensitivity>target_sensitivity:\n",
    "                    print(\"N=\",N)\n",
    "                    print(\"k=\",k)\n",
    "                    print(\"randmx\",randmx)\n",
    "                    print(\"dist_percent\",dist_percent)\n",
    "                    print(\"f1\",f1_score)\n",
    "                    print(\"sensitivity\",sensitivity)\n",
    "                    print(\"specificity\",specificity)\n",
    "                    print(\"*************************************\")\n",
    "    #             if f1_score>=.25:\n",
    "    #                 break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randmx =  1\n",
      "dist_percent =  0.9\n",
      "KNNOR\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'F1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-cb9b52a6e1dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#             print(k1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#             print(v1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"F1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#             print(v1[check][\"F1\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'F1'"
     ]
    }
   ],
   "source": [
    "\n",
    "# get best\n",
    "\n",
    "print(\"randmx = \",randmx)\n",
    "print(\"dist_percent = \",dist_percent)\n",
    "import math\n",
    "# checks=[\"MLP\",\"RF\",\"GNB\",\"SVM\"]\n",
    "checks=[\"MLP\"]\n",
    "\n",
    "best_n=None\n",
    "best_k=None\n",
    "best_sensitivity=0\n",
    "best_specificity=0\n",
    "best_f1=0\n",
    "\n",
    "print(\"KNNOR\")\n",
    "\n",
    "\n",
    "for check in checks:\n",
    "    for k,v in results.items():\n",
    "        if k==\"0\":\n",
    "            continue\n",
    "        for k1,v1 in v.items():\n",
    "#             print(k1)\n",
    "#             print(v1)\n",
    "            if math.isnan(v1[check][\"F1\"]):\n",
    "                continue\n",
    "#             print(v1[check][\"F1\"])\n",
    "            if best_f1<v1[check][\"F1\"]:\n",
    "                best_f1=v1[check][\"F1\"]\n",
    "                best_n=k\n",
    "                best_k=k1\n",
    "                best_sensitivity=v1[check][\"sensitivity\"]\n",
    "                best_specificity=v1[check][\"specificity\"]\n",
    "                \n",
    "\n",
    "    print(check,best_f1,\"[N=\",best_n,\",k=\",best_k,\"]\")\n",
    "    print(\"Sensitivity = \",best_sensitivity)\n",
    "    print(\"Specificity = \",best_specificity)    \n",
    "    \n",
    "    best_f1=0\n",
    "    best_k=None\n",
    "    best_n=None\n",
    "    best_sensitivity=0\n",
    "    best_specificity=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "indept_study",
   "language": "python",
   "name": "indept_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
