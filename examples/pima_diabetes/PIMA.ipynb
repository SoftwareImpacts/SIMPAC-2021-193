{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade augmentdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from augmentdata import data_augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cm_others(y_actual,y_predict):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm1 = confusion_matrix(y_actual,y_predict)\n",
    "    print('Confusion Matrix : \\n', cm1)\n",
    "\n",
    "    total1=sum(sum(cm1))\n",
    "    #####from confusion matrix calculate accuracy\n",
    "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "    print ('Accuracy : ', accuracy1)\n",
    "\n",
    "    tn=cm1[0,0]\n",
    "    fp=cm1[0,1]\n",
    "    fn=cm1[1,0]\n",
    "    tp=cm1[1,1]\n",
    "\n",
    "    sensitivity1 = tp/(tp+fn)\n",
    "    print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "    specificity1 = tn/(tn+fp)\n",
    "    print('Specificity : ', specificity1)\n",
    "    recall=sensitivity1\n",
    "    precision=tp/(tp+fp)\n",
    "\n",
    "    print(\"Precision = \",precision)\n",
    "    print(\"Recall = \",recall)\n",
    "\n",
    "    f1_score=2*(precision*recall)/(precision+recall)\n",
    "\n",
    "    print(\"F1 score = \",f1_score)\n",
    "\n",
    "    return sensitivity1,specificity1,f1_score\n",
    "\n",
    "def create_model(weight_path,input_dim):\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    model=None\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=input_dim, \n",
    "                activation='relu')) \n",
    "\n",
    "    model.add(Dense(20, \n",
    "                activation='relu'))\n",
    "    model.add(Dense(20, \n",
    "                activation='relu'))\n",
    "\n",
    "    model.add(Dense(20, \n",
    "                activation='relu'))\n",
    "\n",
    "    model.add(Dense(20, \n",
    "                activation='relu'))\n",
    "    model.add(Dense(20, \n",
    "                activation='relu'))\n",
    "\n",
    "    model.add(Dense(2, \n",
    "                activation='softmax'))\n",
    "    opt=keras.optimizers.Adam(lr=.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model, callbacks_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"data/diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    268\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Outcome\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                   int64\n",
       "Glucose                       int64\n",
       "BloodPressure                 int64\n",
       "SkinThickness                 int64\n",
       "Insulin                       int64\n",
       "BMI                         float64\n",
       "DiabetesPedigreeFunction    float64\n",
       "Age                           int64\n",
       "Outcome                       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here take 20% out for testing\n",
    "np.random.seed(42)\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "train = df[msk]\n",
    "test = df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n",
      "609\n",
      "(159, 9)\n",
      "(609, 9)\n"
     ]
    }
   ],
   "source": [
    "print(len(test))\n",
    "print(len(train))\n",
    "print(test.shape)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    115\n",
      "1     44\n",
      "Name: Outcome, dtype: int64\n",
      "0    385\n",
      "1    224\n",
      "Name: Outcome, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test[\"Outcome\"].value_counts())\n",
    "print(train[\"Outcome\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
      "       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "columns=df.columns\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with no augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results={}\n",
    "smote_results={}\n",
    "\n",
    "adasyn_results={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"0\"]={}\n",
    "smote_results[\"0\"]={}\n",
    "adasyn_results[\"0\"]={}\n",
    "source=train[train.columns[:-1]]\n",
    "print(source.shape)\n",
    "\n",
    "\n",
    "weight_path=\"weights/wt1.hdf5\"\n",
    "model,callbacks_list=create_model(weight_path,source.shape[1])\n",
    "target = list(train[\"Outcome\"])\n",
    "target=pd.get_dummies(target)\n",
    "start=time.time()\n",
    "print(\"Training MLP\")\n",
    "history = model.fit(source.values, target,epochs=200,validation_split=0.2,callbacks=callbacks_list,verbose=0)\n",
    "end=time.time()\n",
    "difference = int(end - start)\n",
    "print(\"Time taken to train = \",difference)\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# load weights\n",
    "model.load_weights(weight_path)\n",
    "# Compile model (required to make predictions)\n",
    "opt=keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "print(\"used model and loaded weights from file\")\n",
    "\n",
    "print(\"Trying MLP\")\n",
    "results[\"0\"][\"MLP\"]={}\n",
    "smote_results[\"0\"][\"MLP\"]={}\n",
    "adasyn_results[\"0\"][\"MLP\"]={}\n",
    "\n",
    "y_actual = list(test[\"Outcome\"])\n",
    "y_actual=pd.get_dummies(y_actual)\n",
    "test_features_only=test[test.columns[:-1]]\n",
    "_, accuracy = model.evaluate(test_features_only.values, y_actual)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "y_actual = test[\"Outcome\"].astype(int)\n",
    "y_actual=np.asarray(y_actual)\n",
    "y_predict=model.predict_classes(test_features_only.values)\n",
    "sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "results[\"0\"][\"MLP\"][\"F1\"]=f1_score\n",
    "results[\"0\"][\"MLP\"][\"sensitivity\"]=sensitivity\n",
    "results[\"0\"][\"MLP\"][\"specificity\"]=specificity\n",
    "\n",
    "smote_results[\"0\"][\"MLP\"][\"F1\"]=f1_score  \n",
    "smote_results[\"0\"][\"MLP\"][\"sensitivity\"]=sensitivity\n",
    "smote_results[\"0\"][\"MLP\"][\"specificity\"]=specificity\n",
    "\n",
    "adasyn_results[\"0\"][\"MLP\"][\"F1\"]=f1_score  \n",
    "adasyn_results[\"0\"][\"MLP\"][\"sensitivity\"]=sensitivity\n",
    "adasyn_results[\"0\"][\"MLP\"][\"specificity\"]=specificity\n",
    "\n",
    "\n",
    "# for other classifiers\n",
    "target = list(train[\"Outcome\"])\n",
    "\n",
    "# Random forest\n",
    "print(\"RF\")\n",
    "results[\"0\"][\"RF\"]={}\n",
    "smote_results[\"0\"][\"RF\"]={}\n",
    "adasyn_results[\"0\"][\"RF\"]={}\n",
    "\n",
    "clf=RandomForestClassifier()\n",
    "clf.fit(source.values,target)\n",
    "y_pred=None\n",
    "y_predict=clf.predict(test_features_only.values)\n",
    "sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "results[\"0\"][\"RF\"][\"F1\"]=f1_score\n",
    "results[\"0\"][\"RF\"][\"sensitivity\"]=sensitivity\n",
    "results[\"0\"][\"RF\"][\"specificity\"]=specificity\n",
    "\n",
    "smote_results[\"0\"][\"RF\"][\"F1\"]=f1_score  \n",
    "smote_results[\"0\"][\"RF\"][\"sensitivity\"]=sensitivity\n",
    "smote_results[\"0\"][\"RF\"][\"specificity\"]=specificity\n",
    "\n",
    "adasyn_results[\"0\"][\"RF\"][\"F1\"]=f1_score  \n",
    "adasyn_results[\"0\"][\"RF\"][\"sensitivity\"]=sensitivity\n",
    "adasyn_results[\"0\"][\"RF\"][\"specificity\"]=specificity\n",
    "\n",
    "#     Gaussian NB\n",
    "gnb = GaussianNB()\n",
    "results[\"0\"][\"GNB\"]={}\n",
    "smote_results[\"0\"][\"GNB\"]={}\n",
    "adasyn_results[\"0\"][\"GNB\"]={}\n",
    "\n",
    "print(\"GNB\")\n",
    "gnb.fit(source.values,target)\n",
    "y_predict = gnb.predict(test_features_only.values)\n",
    "sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "results[\"0\"][\"GNB\"][\"F1\"]=f1_score\n",
    "results[\"0\"][\"GNB\"][\"sensitivity\"]=sensitivity\n",
    "results[\"0\"][\"GNB\"][\"specificity\"]=specificity\n",
    "\n",
    "smote_results[\"0\"][\"GNB\"][\"F1\"]=f1_score  \n",
    "smote_results[\"0\"][\"GNB\"][\"sensitivity\"]=sensitivity\n",
    "smote_results[\"0\"][\"GNB\"][\"specificity\"]=specificity\n",
    "\n",
    "adasyn_results[\"0\"][\"GNB\"][\"F1\"]=f1_score  \n",
    "adasyn_results[\"0\"][\"GNB\"][\"sensitivity\"]=sensitivity\n",
    "adasyn_results[\"0\"][\"GNB\"][\"specificity\"]=specificity\n",
    "\n",
    "#     SVM\n",
    "clf = svm.SVC()\n",
    "results[\"0\"][\"SVM\"]={}\n",
    "smote_results[\"0\"][\"SVM\"]={}\n",
    "adasyn_results[\"0\"][\"SVM\"]={}\n",
    "\n",
    "print(\"SVM\")\n",
    "clf.fit(source.values,target)\n",
    "y_predict=clf.predict(test_features_only.values)\n",
    "sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "results[\"0\"][\"SVM\"][\"F1\"]=f1_score\n",
    "results[\"0\"][\"SVM\"][\"sensitivity\"]=sensitivity\n",
    "results[\"0\"][\"SVM\"][\"specificity\"]=specificity\n",
    "\n",
    "smote_results[\"0\"][\"SVM\"][\"F1\"]=f1_score  \n",
    "smote_results[\"0\"][\"SVM\"][\"sensitivity\"]=sensitivity\n",
    "smote_results[\"0\"][\"SVM\"][\"specificity\"]=specificity\n",
    "\n",
    "adasyn_results[\"0\"][\"SVM\"][\"F1\"]=f1_score  \n",
    "adasyn_results[\"0\"][\"SVM\"][\"sensitivity\"]=sensitivity\n",
    "adasyn_results[\"0\"][\"SVM\"][\"specificity\"]=specificity\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)\n",
    "\n",
    "print(smote_results)\n",
    "print(adasyn_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Outcome\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train  with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_range=[50,100,160]\n",
    "for N in N_range:\n",
    "    results[N]={}\n",
    "    smote_results[N]={}\n",
    "    adasyn_results[N]={}\n",
    "    \n",
    "    k_range=[1,2,5,10]\n",
    "    for k in k_range:\n",
    "        results[N][k]={}\n",
    "        smote_results[N][k]={}  \n",
    "        adasyn_results[N][k]={}\n",
    "        \n",
    "        results[N][k][\"MLP\"]={}\n",
    "        smote_results[N][k][\"MLP\"]={}  \n",
    "        adasyn_results[N][k][\"MLP\"]={}\n",
    "\n",
    "        results[N][k][\"RF\"]={}\n",
    "        smote_results[N][k][\"RF\"]={}  \n",
    "        adasyn_results[N][k][\"RF\"]={}\n",
    "        \n",
    "        results[N][k][\"GNB\"]={}\n",
    "        smote_results[N][k][\"GNB\"]={}  \n",
    "        adasyn_results[N][k][\"GNB\"]={}\n",
    "\n",
    "        results[N][k][\"SVM\"]={}\n",
    "        smote_results[N][k][\"SVM\"]={}  \n",
    "        adasyn_results[N][k][\"SVM\"]={}\n",
    "\n",
    "\n",
    "        class_index=1\n",
    "        randmx=.001\n",
    "        dist_percent=0.2\n",
    "\n",
    "        \n",
    "        daug = data_augment.DataAugment()\n",
    "        print(\"randmx = \",randmx)\n",
    "        now = time.time()\n",
    "        [Data_a,Ext_d,Ext_not]=daug.augment(data=train.values,k=k,class_ind=class_index,N=N,\n",
    "                                            randmx=randmx,dist_percent=dist_percent)\n",
    "        later = time.time()\n",
    "        difference = int(later - now)\n",
    "        print(\"Time taken to augment = \",difference)\n",
    "        print(len(Data_a))\n",
    "\n",
    "        train_aug=pd.DataFrame(data=Data_a,index=None,    # values                \n",
    "                columns=columns)      \n",
    "\n",
    "        print(\"After augmentation of \",N,\" items with \",k,\" neighbors\")\n",
    "\n",
    "        print(train_aug[\"Outcome\"].value_counts())\n",
    "        source=train_aug[train_aug.columns[:-1]]\n",
    "        target = list(train_aug[\"Outcome\"])\n",
    "        target=pd.get_dummies(target)\n",
    "\n",
    "        weight_path=\"weights/\"+str(N)+\"_\"+str(k)+\"_wt1.hdf5\"    \n",
    "        model,callbacks_list=create_model(weight_path,source.shape[1])\n",
    "        start=time.time()\n",
    "        print(\"Training model for N = \",N,\" k = \",k)\n",
    "        history = model.fit(source.values, target,epochs=300,validation_split=0.2,callbacks=callbacks_list,verbose=0)\n",
    "        end=time.time()\n",
    "        difference = int(end - start)\n",
    "        print(\"Time taken to train = \",difference)\n",
    "\n",
    "\n",
    "\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train','validation'], loc='upper left')\n",
    "        plt.show()    \n",
    "\n",
    "        # load weights\n",
    "        model.load_weights(weight_path)\n",
    "        # Compile model (required to make predictions)\n",
    "        opt=keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "        print(\"used model and loaded weights from file\")    \n",
    "        print(\"Test distribution\")\n",
    "        print(test[\"Outcome\"].value_counts())\n",
    "        print(\"Trying MLP\")\n",
    "        y_actual = list(test[\"Outcome\"])\n",
    "        y_actual=pd.get_dummies(y_actual)\n",
    "        test_features_only=test[test.columns[:-1]]\n",
    "        _, accuracy = model.evaluate(test_features_only.values, y_actual)\n",
    "        print('Accuracy: %.2f' % (accuracy*100))            \n",
    "\n",
    "\n",
    "        y_actual = test[\"Outcome\"].astype(int)\n",
    "        y_actual=np.asarray(y_actual)\n",
    "        y_predict=model.predict_classes(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        results[N][k][\"MLP\"][\"F1\"]=f1_score \n",
    "        results[N][k][\"MLP\"][\"sensitivity\"]=sensitivity \n",
    "        results[N][k][\"MLP\"][\"specificity\"]=specificity         \n",
    "        \n",
    "\n",
    "\n",
    "        # for other classifiers\n",
    "        target = list(train_aug[\"Outcome\"])\n",
    "        source=train_aug[train_aug.columns[:-1]]\n",
    "        test_features_only=test[test.columns[:-1]]\n",
    "\n",
    "        # Random forest\n",
    "        print(\"RF\")\n",
    "        clf=RandomForestClassifier()\n",
    "        clf.fit(source.values,target)\n",
    "        y_predict=clf.predict(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        results[N][k][\"RF\"][\"F1\"]=f1_score \n",
    "        results[N][k][\"RF\"][\"sensitivity\"]=sensitivity \n",
    "        results[N][k][\"RF\"][\"specificity\"]=specificity         \n",
    "                \n",
    "\n",
    "    #     Gaussian NB\n",
    "        print(\"GNB\")\n",
    "        gnb = GaussianNB()\n",
    "        gnb.fit(source.values,target)\n",
    "        y_predict = gnb.predict(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        results[N][k][\"GNB\"][\"F1\"]=f1_score \n",
    "        results[N][k][\"GNB\"][\"sensitivity\"]=sensitivity \n",
    "        results[N][k][\"GNB\"][\"specificity\"]=specificity         \n",
    "                        \n",
    "        \n",
    "\n",
    "    #     SVM\n",
    "        print(\"SVM\")\n",
    "        clf = svm.SVC()\n",
    "        clf.fit(source.values,target)\n",
    "        y_predict=clf.predict(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        results[N][k][\"SVM\"][\"F1\"]=f1_score \n",
    "        results[N][k][\"SVM\"][\"sensitivity\"]=sensitivity \n",
    "        results[N][k][\"SVM\"][\"specificity\"]=specificity         \n",
    "                                \n",
    "        \n",
    "        \n",
    "        \n",
    "        # this part for SMOTE\n",
    "        print(\"******Starting SMOTE***********\")\n",
    "        source=train[train.columns[:-1]]\n",
    "        print(\"Shape of training \",source.shape)\n",
    "\n",
    "        y_train=train[\"Outcome\"].values\n",
    "        num_minority=sum(y_train==class_index)\n",
    "        num_majority=len(y_train)-num_minority\n",
    "        print(num_majority,num_minority)\n",
    "        \n",
    "        samp_strategy=(num_minority+N)/num_majority\n",
    "        print(samp_strategy)\n",
    "        now = time.time()\n",
    "        sm = SMOTE(random_state=2,k_neighbors=k,sampling_strategy=samp_strategy)\n",
    "        X_train_res, y_train_res = sm.fit_sample(source, y_train.ravel())\n",
    "        source=X_train_res\n",
    "\n",
    "        later = time.time()\n",
    "        difference = int(later - now)\n",
    "        print(\"Time taken to augment by SMOTE = \",difference)\n",
    "\n",
    "        print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
    "        print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
    "\n",
    "        print(\"After OverSampling, counts of label {}: {}\".format(class_index,sum(y_train_res==1)))\n",
    "        print(\"After OverSampling, counts of other label: {}\".format(len(y_train_res)-sum(y_train_res==class_index)))  \n",
    "        \n",
    "        weight_path=\"weights/smote\"+\"_\"+str(N)+\"_\"+str(k)+\"_wt1.hdf5\"    \n",
    "        model,callbacks_list=create_model(weight_path,X_train_res.shape[1])\n",
    "        start=time.time()\n",
    "        print(\"Training model\")\n",
    "        target=pd.get_dummies(y_train_res)\n",
    "        history = model.fit(X_train_res.values, target,epochs=300,validation_split=0.2,callbacks=callbacks_list,verbose=0)\n",
    "        end=time.time()\n",
    "        difference = int(end - start)\n",
    "        print(\"Time taken to train = \",difference) \n",
    "\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train','validation'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "        # load weights\n",
    "        model.load_weights(weight_path)\n",
    "        # Compile model (required to make predictions)\n",
    "        opt=keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "        print(\"used model and loaded weights from file\")    \n",
    "        print(\"Trying MLP\")\n",
    "        y_actual = list(test[\"Outcome\"])\n",
    "        y_actual=pd.get_dummies(y_actual)\n",
    "        test_features_only=test[test.columns[:-1]]\n",
    "        _, accuracy = model.evaluate(test_features_only.values, y_actual)\n",
    "        print('Accuracy: %.2f' % (accuracy*100)) \n",
    "\n",
    "\n",
    "\n",
    "        y_actual = test[\"Outcome\"].astype(int)\n",
    "        y_actual=np.asarray(y_actual)\n",
    "        y_predict=model.predict_classes(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        smote_results[N][k][\"MLP\"][\"F1\"]=f1_score         \n",
    "        smote_results[N][k][\"MLP\"][\"sensitivity\"]=sensitivity                 \n",
    "        smote_results[N][k][\"MLP\"][\"specificity\"]=specificity                 \n",
    "\n",
    "        # for other classifiers\n",
    "        target = y_train_res\n",
    "        source=X_train_res\n",
    "        test_features_only=test[test.columns[:-1]]\n",
    "\n",
    "        # Random forest\n",
    "        print(\"RF\")\n",
    "        clf=RandomForestClassifier()\n",
    "        clf.fit(source.values,target)\n",
    "        y_predict=clf.predict(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        smote_results[N][k][\"RF\"][\"F1\"]=f1_score         \n",
    "        smote_results[N][k][\"RF\"][\"sensitivity\"]=sensitivity                 \n",
    "        smote_results[N][k][\"RF\"][\"specificity\"]=specificity\n",
    "\n",
    "    #     Gaussian NB\n",
    "        print(\"GNB\")\n",
    "        gnb = GaussianNB()\n",
    "        gnb.fit(source.values,target)\n",
    "        y_predict = gnb.predict(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        smote_results[N][k][\"GNB\"][\"F1\"]=f1_score         \n",
    "        smote_results[N][k][\"GNB\"][\"sensitivity\"]=sensitivity                 \n",
    "        smote_results[N][k][\"GNB\"][\"specificity\"]=specificity\n",
    "\n",
    "    #     SVM\n",
    "        print(\"SVM\")\n",
    "        clf = svm.SVC()\n",
    "        clf.fit(source.values,target)\n",
    "        y_predict=clf.predict(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        smote_results[N][k][\"SVM\"][\"F1\"]=f1_score         \n",
    "        smote_results[N][k][\"SVM\"][\"sensitivity\"]=sensitivity                 \n",
    "        smote_results[N][k][\"SVM\"][\"specificity\"]=specificity\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # this part for ADASYN\n",
    "        print(\"******Starting ADASYN***********\")\n",
    "        source=train[train.columns[:-1]]\n",
    "        print(\"Shape of training \",source.shape)\n",
    "\n",
    "        y_train=train[\"Outcome\"].values\n",
    "        num_minority=sum(y_train==class_index)\n",
    "        num_majority=len(y_train)-num_minority\n",
    "        print(num_majority,num_minority)\n",
    "        \n",
    "        samp_strategy=(num_minority+N)/num_majority\n",
    "        print(samp_strategy)\n",
    "        now = time.time()\n",
    "#         ad = ADASYN(random_state=2,n_neighbors=k,sampling_strategy=samp_strategy)\n",
    "        ad = ADASYN(random_state=2,n_neighbors=k)\n",
    "        X_train_res, y_train_res = ad.fit_sample(source, y_train.ravel())\n",
    "        source=X_train_res\n",
    "\n",
    "        later = time.time()\n",
    "        difference = int(later - now)\n",
    "        print(\"Time taken to augment by ADASYN = \",difference)\n",
    "\n",
    "        print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
    "        print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
    "\n",
    "        print(\"After OverSampling, counts of label {}: {}\".format(class_index,sum(y_train_res==1)))\n",
    "        print(\"After OverSampling, counts of other label: {}\".format(len(y_train_res)-sum(y_train_res==class_index)))  \n",
    "        \n",
    "        weight_path=\"weights/adasyn\"+\"_\"+str(N)+\"_\"+str(k)+\"_wt1.hdf5\"    \n",
    "        model,callbacks_list=create_model(weight_path,X_train_res.shape[1])\n",
    "        start=time.time()\n",
    "        print(\"Training model\")\n",
    "        target=pd.get_dummies(y_train_res)\n",
    "        history = model.fit(X_train_res.values, target,epochs=300,validation_split=0.2,callbacks=callbacks_list,verbose=0)\n",
    "        end=time.time()\n",
    "        difference = int(end - start)\n",
    "        print(\"Time taken to train = \",difference) \n",
    "\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train','validation'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "        # load weights\n",
    "        model.load_weights(weight_path)\n",
    "        # Compile model (required to make predictions)\n",
    "        opt=keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "        print(\"used model and loaded weights from file\")    \n",
    "        print(\"Trying MLP\")\n",
    "        y_actual = list(test[\"Outcome\"])\n",
    "        y_actual=pd.get_dummies(y_actual)\n",
    "        test_features_only=test[test.columns[:-1]]\n",
    "        _, accuracy = model.evaluate(test_features_only.values, y_actual)\n",
    "        print('Accuracy: %.2f' % (accuracy*100)) \n",
    "\n",
    "\n",
    "\n",
    "        y_actual = test[\"Outcome\"].astype(int)\n",
    "        y_actual=np.asarray(y_actual)\n",
    "        y_predict=model.predict_classes(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        adasyn_results[N][k][\"MLP\"][\"F1\"]=f1_score         \n",
    "        adasyn_results[N][k][\"MLP\"][\"sensitivity\"]=sensitivity                 \n",
    "        adasyn_results[N][k][\"MLP\"][\"specificity\"]=specificity\n",
    "\n",
    "        # for other classifiers\n",
    "        target = y_train_res\n",
    "        source=X_train_res\n",
    "        test_features_only=test[test.columns[:-1]]\n",
    "\n",
    "        # Random forest\n",
    "        print(\"RF\")\n",
    "        clf=RandomForestClassifier()\n",
    "        clf.fit(source.values,target)\n",
    "        y_predict=clf.predict(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        adasyn_results[N][k][\"RF\"][\"F1\"]=f1_score         \n",
    "        adasyn_results[N][k][\"RF\"][\"sensitivity\"]=sensitivity                 \n",
    "        adasyn_results[N][k][\"RF\"][\"specificity\"]=specificity\n",
    "\n",
    "    #     Gaussian NB\n",
    "        print(\"GNB\")\n",
    "        gnb = GaussianNB()\n",
    "        gnb.fit(source.values,target)\n",
    "        y_predict = gnb.predict(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        adasyn_results[N][k][\"GNB\"][\"F1\"]=f1_score         \n",
    "        adasyn_results[N][k][\"GNB\"][\"sensitivity\"]=sensitivity                 \n",
    "        adasyn_results[N][k][\"GNB\"][\"specificity\"]=specificity\n",
    "\n",
    "    #     SVM\n",
    "        print(\"SVM\")\n",
    "        clf = svm.SVC()\n",
    "        clf.fit(source.values,target)\n",
    "        y_predict=clf.predict(test_features_only.values)\n",
    "        sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "        adasyn_results[N][k][\"SVM\"][\"F1\"]=f1_score         \n",
    "        adasyn_results[N][k][\"SVM\"][\"sensitivity\"]=sensitivity                 \n",
    "        adasyn_results[N][k][\"SVM\"][\"specificity\"]=specificity        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best\n",
    "\n",
    "print(\"randmx = \",randmx)\n",
    "print(\"dist_percent = \",dist_percent)\n",
    "import math\n",
    "checks=[\"MLP\",\"RF\",\"GNB\",\"SVM\"]\n",
    "# checks=[\"GNB\"]\n",
    "\n",
    "best_n=None\n",
    "best_k=None\n",
    "best_sensitivity=0\n",
    "best_specificity=0\n",
    "best_f1=0\n",
    "\n",
    "print(\"KNNOR\")\n",
    "\n",
    "\n",
    "for check in checks:\n",
    "    for k,v in results.items():\n",
    "        if k==\"0\":\n",
    "            continue\n",
    "        for k1,v1 in v.items():\n",
    "#             print(k1)\n",
    "#             print(v1)\n",
    "            if math.isnan(v1[check][\"F1\"]):\n",
    "                continue\n",
    "#             print(v1[check][\"F1\"])\n",
    "            if best_f1<v1[check][\"F1\"]:\n",
    "                best_f1=v1[check][\"F1\"]\n",
    "                best_n=k\n",
    "                best_k=k1\n",
    "                best_sensitivity=v1[check][\"sensitivity\"]\n",
    "                best_specificity=v1[check][\"specificity\"]\n",
    "                \n",
    "\n",
    "    print(check,best_f1,\"[N=\",best_n,\",k=\",best_k,\"]\")\n",
    "    print(\"Sensitivity = \",best_sensitivity)\n",
    "    print(\"Specificity = \",best_specificity)    \n",
    "    \n",
    "    best_f1=0\n",
    "    best_k=None\n",
    "    best_n=None\n",
    "    best_sensitivity=0\n",
    "    best_specificity=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best\n",
    "\n",
    "print(\"randmx = \",randmx)\n",
    "print(\"dist_percent = \",dist_percent)\n",
    "import math\n",
    "checks=[\"MLP\",\"RF\",\"GNB\",\"SVM\"]\n",
    "# checks=[\"GNB\"]\n",
    "\n",
    "best_n=None\n",
    "best_k=None\n",
    "best_sensitivity=0\n",
    "best_specificity=0\n",
    "best_f1=0\n",
    "\n",
    "print(\"SMOTE\")\n",
    "\n",
    "\n",
    "for check in checks:\n",
    "    for k,v in smote_results.items():\n",
    "        if k==\"0\":\n",
    "            continue\n",
    "        for k1,v1 in v.items():\n",
    "#             print(k1)\n",
    "#             print(v1)\n",
    "            if math.isnan(v1[check][\"F1\"]):\n",
    "                continue\n",
    "#             print(v1[check][\"F1\"])\n",
    "            if best_f1<v1[check][\"F1\"]:\n",
    "                best_f1=v1[check][\"F1\"]\n",
    "                best_n=k\n",
    "                best_k=k1\n",
    "                best_sensitivity=v1[check][\"sensitivity\"]\n",
    "                best_specificity=v1[check][\"specificity\"]                \n",
    "\n",
    "    print(check,best_f1,\"[N=\",best_n,\",k=\",best_k,\"]\")\n",
    "    print(\"Sensitivity = \",best_sensitivity)\n",
    "    print(\"Specificity = \",best_specificity)\n",
    "    \n",
    "    best_f1=0\n",
    "    best_k=None\n",
    "    best_n=None\n",
    "    best_sensitivity=0\n",
    "    best_specificity=0\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best\n",
    "\n",
    "print(\"randmx = \",randmx)\n",
    "print(\"dist_percent = \",dist_percent)\n",
    "import math\n",
    "checks=[\"MLP\",\"RF\",\"GNB\",\"SVM\"]\n",
    "# checks=[\"GNB\"]\n",
    "\n",
    "best_n=None\n",
    "best_k=None\n",
    "best_sensitivity=0\n",
    "best_specificity=0\n",
    "best_f1=0\n",
    "\n",
    "print(\"ADASYN\")\n",
    "\n",
    "\n",
    "for check in checks:\n",
    "    for k,v in adasyn_results.items():\n",
    "        if k==\"0\":\n",
    "            continue\n",
    "        for k1,v1 in v.items():\n",
    "#             print(k1)\n",
    "#             print(v1)\n",
    "            if math.isnan(v1[check][\"F1\"]):\n",
    "                continue\n",
    "#             print(v1[check][\"F1\"])\n",
    "            if best_f1<v1[check][\"F1\"]:\n",
    "                best_f1=v1[check][\"F1\"]\n",
    "                best_n=k\n",
    "                best_k=k1\n",
    "                best_sensitivity=v1[check][\"sensitivity\"]\n",
    "                best_specificity=v1[check][\"specificity\"]                \n",
    "\n",
    "    print(check,best_f1,\"[N=\",best_n,\",k=\",best_k,\"]\")\n",
    "    print(\"Sensitivity = \",best_sensitivity)\n",
    "    print(\"Specificity = \",best_specificity)\n",
    "    \n",
    "    best_f1=0\n",
    "    best_k=None\n",
    "    best_n=None\n",
    "    best_sensitivity=0\n",
    "    best_specificity=0\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re train as required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [13 31]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5849056603773585\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [12 32]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5079365079365079\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5981308411214953\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [13 31]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5849056603773585\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[83 32]\n",
      " [13 31]]\n",
      "Accuracy :  0.7169811320754716\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7217391304347827\n",
      "Precision =  0.49206349206349204\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5794392523364486\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [12 32]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5079365079365079\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5981308411214953\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [12 32]]\n",
      "Accuracy :  0.7358490566037735\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5161290322580645\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.6037735849056604\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [13 31]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5849056603773585\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [12 32]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5079365079365079\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5981308411214953\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [12 32]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5079365079365079\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5981308411214953\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [13 31]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5849056603773585\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [13 31]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5849056603773585\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[83 32]\n",
      " [12 32]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7217391304347827\n",
      "Precision =  0.5\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5925925925925926\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [13 31]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5849056603773585\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [12 32]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5079365079365079\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5981308411214953\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [12 32]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5079365079365079\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5981308411214953\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[83 32]\n",
      " [12 32]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7217391304347827\n",
      "Precision =  0.5\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5925925925925926\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[83 32]\n",
      " [12 32]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7217391304347827\n",
      "Precision =  0.5\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5925925925925926\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [12 32]]\n",
      "Accuracy :  0.7358490566037735\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5161290322580645\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.6037735849056604\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [13 31]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5849056603773585\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [12 32]]\n",
      "Accuracy :  0.7358490566037735\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5161290322580645\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.6037735849056604\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [13 31]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5849056603773585\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [13 31]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5849056603773585\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [13 31]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5849056603773585\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [13 31]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5849056603773585\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [13 31]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5849056603773585\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [13 31]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5849056603773585\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [12 32]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5079365079365079\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5981308411214953\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [13 31]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5849056603773585\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [12 32]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5079365079365079\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5981308411214953\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [12 32]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5079365079365079\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5981308411214953\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[83 32]\n",
      " [12 32]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7217391304347827\n",
      "Precision =  0.5\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5925925925925926\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [12 32]]\n",
      "Accuracy :  0.7358490566037735\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5161290322580645\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.6037735849056604\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[83 32]\n",
      " [12 32]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7217391304347827\n",
      "Precision =  0.5\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5925925925925926\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[83 32]\n",
      " [12 32]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7217391304347827\n",
      "Precision =  0.5\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5925925925925926\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [12 32]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5079365079365079\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5981308411214953\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [12 32]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5079365079365079\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5981308411214953\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [12 32]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5079365079365079\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5981308411214953\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [12 32]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5079365079365079\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5981308411214953\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [13 31]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5849056603773585\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [13 31]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5849056603773585\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [12 32]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5079365079365079\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5981308411214953\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [12 32]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5079365079365079\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5981308411214953\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [12 32]]\n",
      "Accuracy :  0.7358490566037735\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5161290322580645\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.6037735849056604\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [12 32]]\n",
      "Accuracy :  0.7358490566037735\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5161290322580645\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.6037735849056604\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[83 32]\n",
      " [13 31]]\n",
      "Accuracy :  0.7169811320754716\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7217391304347827\n",
      "Precision =  0.49206349206349204\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5794392523364486\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [13 31]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5849056603773585\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[83 32]\n",
      " [13 31]]\n",
      "Accuracy :  0.7169811320754716\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7217391304347827\n",
      "Precision =  0.49206349206349204\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5794392523364486\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [12 32]]\n",
      "Accuracy :  0.7358490566037735\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5161290322580645\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.6037735849056604\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [13 31]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5849056603773585\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [13 31]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5849056603773585\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [13 31]]\n",
      "Accuracy :  0.7232704402515723\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5849056603773585\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [12 32]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5079365079365079\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5981308411214953\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [12 32]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5079365079365079\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5981308411214953\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [12 32]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5079365079365079\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5981308411214953\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [12 32]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5079365079365079\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5981308411214953\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[84 31]\n",
      " [12 32]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7272727272727273\n",
      "Specificity :  0.7304347826086957\n",
      "Precision =  0.5079365079365079\n",
      "Recall =  0.7272727272727273\n",
      "F1 score =  0.5981308411214953\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n",
      "Time taken to augment =  0\n",
      "769\n",
      "After augmentation of  160  items with  2  neighbors\n",
      "GNB\n",
      "Confusion Matrix : \n",
      " [[85 30]\n",
      " [13 31]]\n",
      "Accuracy :  0.7295597484276729\n",
      "Sensitivity :  0.7045454545454546\n",
      "Specificity :  0.7391304347826086\n",
      "Precision =  0.5081967213114754\n",
      "Recall =  0.7045454545454546\n",
      "F1 score =  0.5904761904761906\n",
      "randmx =  0.5\n",
      "dist_percent =  0.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-8e55632e1e16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                     [Data_a,Ext_d,Ext_not]=daug.augment(data=train.values,k=k,class_ind=class_index,N=N,\n\u001b[0;32m---> 45\u001b[0;31m                                                         randmx=randmx,dist_percent=dist_percent)\n\u001b[0m\u001b[1;32m     46\u001b[0m                     \u001b[0mlater\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0mdifference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlater\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/venv_vasic_gpu/lib/python3.6/site-packages/augmentdata/data_augment.py\u001b[0m in \u001b[0;36maugment\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    249\u001b[0m                                                 \u001b[0;31m# for testing the artificial data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                                                 \u001b[0mcan_use\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neighbors_to_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m                                                 \u001b[0;31m# print(\"Usability \",can_use)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/venv_vasic_gpu/lib/python3.6/site-packages/augmentdata/test_nn.py\u001b[0m in \u001b[0;36mpredict_classification\u001b[0;34m(train, test_row, num_neighbors, test_val)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Make a classification prediction with neighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neighbors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mneighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moutput_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# print(output_values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/venv_vasic_gpu/lib/python3.6/site-packages/augmentdata/test_nn.py\u001b[0m in \u001b[0;36mget_neighbors\u001b[0;34m(train, test_row, num_neighbors)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_row\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuclidean_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/venv_vasic_gpu/lib/python3.6/site-packages/augmentdata/test_nn.py\u001b[0m in \u001b[0;36meuclidean_distance\u001b[0;34m(row1, row2)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0mdistance\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrow2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "to_break=False\n",
    "while True:\n",
    "    for dist_percent in [0.5]: \n",
    "        for randmx in [0.5]:\n",
    "            N_range=[160]\n",
    "            for N in N_range:\n",
    "                results[N]={}\n",
    "                smote_results[N]={}\n",
    "                adasyn_results[N]={}\n",
    "\n",
    "                k_range=[2]\n",
    "                for k in k_range:\n",
    "                    results[N][k]={}\n",
    "                    smote_results[N][k]={}  \n",
    "                    adasyn_results[N][k]={}\n",
    "\n",
    "                    results[N][k][\"MLP\"]={}\n",
    "                    smote_results[N][k][\"MLP\"]={}  \n",
    "                    adasyn_results[N][k][\"MLP\"]={}\n",
    "\n",
    "                    results[N][k][\"RF\"]={}\n",
    "                    smote_results[N][k][\"RF\"]={}  \n",
    "                    adasyn_results[N][k][\"RF\"]={}\n",
    "\n",
    "                    results[N][k][\"GNB\"]={}\n",
    "                    smote_results[N][k][\"GNB\"]={}  \n",
    "                    adasyn_results[N][k][\"GNB\"]={}\n",
    "\n",
    "                    results[N][k][\"SVM\"]={}\n",
    "                    smote_results[N][k][\"SVM\"]={}  \n",
    "                    adasyn_results[N][k][\"SVM\"]={}\n",
    "\n",
    "\n",
    "                    class_index=1\n",
    "    #                 randmx=.25\n",
    "    #                 dist_percent=0.9\n",
    "\n",
    "\n",
    "                    daug = data_augment.DataAugment()\n",
    "                    print(\"randmx = \",randmx)\n",
    "                    print(\"dist_percent = \",dist_percent)\n",
    "            \n",
    "                    now = time.time()\n",
    "                    [Data_a,Ext_d,Ext_not]=daug.augment(data=train.values,k=k,class_ind=class_index,N=N,\n",
    "                                                        randmx=randmx,dist_percent=dist_percent)\n",
    "                    later = time.time()\n",
    "                    difference = int(later - now)\n",
    "                    print(\"Time taken to augment = \",difference)\n",
    "                    print(len(Data_a))\n",
    "\n",
    "                    train_aug=pd.DataFrame(data=Data_a,index=None,    # values                \n",
    "                            columns=columns)      \n",
    "\n",
    "                    print(\"After augmentation of \",N,\" items with \",k,\" neighbors\")\n",
    "\n",
    "#                     print(train_aug[\"Outcome\"].value_counts())\n",
    "#                     source=train_aug[train_aug.columns[:-1]]\n",
    "#                     target = list(train_aug[\"Outcome\"])\n",
    "#                     target=pd.get_dummies(target)\n",
    "\n",
    "#                     weight_path=\"weights/\"+str(N)+\"_\"+str(k)+\"_wt1.hdf5\"    \n",
    "#                     model,callbacks_list=create_model(weight_path,source.shape[1])\n",
    "#                     start=time.time()\n",
    "#                     print(\"Training model for N = \",N,\" k = \",k)\n",
    "#                     history = model.fit(source.values, target,epochs=300,validation_split=0.2,callbacks=callbacks_list,verbose=0)\n",
    "#                     end=time.time()\n",
    "#                     difference = int(end - start)\n",
    "#                     print(\"Time taken to train = \",difference)\n",
    "\n",
    "\n",
    "\n",
    "#                     plt.plot(history.history['loss'])\n",
    "#                     plt.plot(history.history['val_loss'])\n",
    "#                     plt.title('model loss')\n",
    "#                     plt.ylabel('loss')\n",
    "#                     plt.xlabel('epoch')\n",
    "#                     plt.legend(['train','validation'], loc='upper left')\n",
    "#                     plt.show()    \n",
    "\n",
    "#                     # load weights\n",
    "#                     model.load_weights(weight_path)\n",
    "#                     # Compile model (required to make predictions)\n",
    "#                     opt=keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "#                     model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "#                     print(\"used model and loaded weights from file\")    \n",
    "#                     print(\"Test distribution\")\n",
    "#                     print(test[\"Outcome\"].value_counts())\n",
    "#                     print(\"Trying MLP\")\n",
    "#                     y_actual = list(test[\"Outcome\"])\n",
    "#                     y_actual=pd.get_dummies(y_actual)\n",
    "#                     test_features_only=test[test.columns[:-1]]\n",
    "#                     _, accuracy = model.evaluate(test_features_only.values, y_actual)\n",
    "#                     print('Accuracy: %.2f' % (accuracy*100))            \n",
    "\n",
    "\n",
    "                    y_actual = test[\"Outcome\"].astype(int)\n",
    "                    y_actual=np.asarray(y_actual)\n",
    "#                     y_predict=model.predict_classes(test_features_only.values)\n",
    "#                     sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "#                     results[N][k][\"MLP\"][\"F1\"]=f1_score \n",
    "#                     results[N][k][\"MLP\"][\"sensitivity\"]=sensitivity \n",
    "#                     results[N][k][\"MLP\"][\"specificity\"]=specificity    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #         # for other classifiers\n",
    "                    target = list(train_aug[\"Outcome\"])\n",
    "                    source=train_aug[train_aug.columns[:-1]]\n",
    "                    test_features_only=test[test.columns[:-1]]\n",
    "\n",
    "            #         # Random forest\n",
    "#                     print(\"RF\")\n",
    "#                     clf=RandomForestClassifier()\n",
    "#                     clf.fit(source.values,target)\n",
    "#                     y_predict=clf.predict(test_features_only.values)\n",
    "#                     sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "#                     results[N][k][\"RF\"][\"F1\"]=f1_score \n",
    "#                     results[N][k][\"RF\"][\"sensitivity\"]=sensitivity \n",
    "#                     results[N][k][\"RF\"][\"specificity\"]=specificity         \n",
    "\n",
    "\n",
    "            #     #     Gaussian NB\n",
    "                    print(\"GNB\")\n",
    "                    gnb = GaussianNB()\n",
    "                    gnb.fit(source.values,target)\n",
    "                    y_predict = gnb.predict(test_features_only.values)\n",
    "                    sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "                    results[N][k][\"GNB\"][\"F1\"]=f1_score \n",
    "                    results[N][k][\"GNB\"][\"sensitivity\"]=sensitivity \n",
    "                    results[N][k][\"GNB\"][\"specificity\"]=specificity         \n",
    "\n",
    "\n",
    "\n",
    "            #     #     SVM\n",
    "#                     print(\"SVM\")\n",
    "#                     clf = svm.SVC()\n",
    "#                     clf.fit(source.values,target)\n",
    "#                     y_predict=clf.predict(test_features_only.values)\n",
    "#                     sensitivity,specificity,f1_score=check_cm_others(y_actual,y_predict)\n",
    "#                     results[N][k][\"SVM\"][\"F1\"]=f1_score \n",
    "#                     results[N][k][\"SVM\"][\"sensitivity\"]=sensitivity \n",
    "#                     results[N][k][\"SVM\"][\"specificity\"]=specificity         \n",
    "\n",
    "\n",
    "\n",
    "                    if f1_score>0.62:\n",
    "                        to_break=True\n",
    "                if to_break:\n",
    "                    break\n",
    "            if to_break:\n",
    "                break\n",
    "        if to_break:\n",
    "            break\n",
    "    if to_break:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
